---
- name: Load admin credentials into interactive shells
  become: true
  ansible.builtin.template:
    src: kubernetes-admin.sh.j2
    dest: /etc/profile.d/kubernetes-admin.sh
    owner: root
    group: root
    mode: u=rw,go=r

- name: Check reachability of local kube-apiserver
  ansible.builtin.wait_for:
    port: 6443
    timeout: 5
  ignore_errors: true
  register: local_kube_apiserver_status

- name: Check reachability of load-balanced kube-apiserver
  when: local_kube_apiserver_status is failed
  vars:
    host: "{{ networking_fixed_ip }}:{{ k8s_apiserver_frontend_port }}"
  ansible.builtin.uri:
    url: "https://{{ host }}"
    status_code: 403
    validate_certs: false  # self-signed certificate
    timeout: 3
  register: lb_kube_apiserver_status
  until: lb_kube_apiserver_status.status == 403
  retries: 3
  ignore_errors: true

- name: Make sure 'etc_dir' exists
  delegate_to: "{{ groups['orchestrator'] | first }}"
  become: false
  ansible.builtin.file:
    path: "{{ etc_dir }}"
    state: directory
    mode: 0755

- name: Create parent directories for PKI directories
  become: true
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: u=rwx,go=rx
  loop:
  - /etc/kubernetes
  - /var/lib/kubelet

# The focus of the following two tasks is on fixing the permissions.
- name: Create PKI directories
  become: true
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: u=rwx,go-rwx
  loop:
  - /etc/kubernetes/pki
  - /etc/kubernetes/pki/etcd
  - /var/lib/kubelet/pki

- name: Ensure availability of Kubernetes cluster root CA key in Vault
  delegate_to: "{{ groups['orchestrator'] | first }}"
  environment:
    ANSIBLE_HASHI_VAULT_URL: "{{ lookup('env', 'VAULT_ADDR') }}"
    ANSIBLE_HASHI_VAULT_CA_CERT: "{{ lookup('env', 'VAULT_CACERT') }}"
  block:
  - name: "Check availability of CA key in Vault: Fetch key"
    no_log: true  # prevent private key from being exposed
    community.hashi_vault.vault_kv2_get:
      path: k8s-pki/cluster-root-ca
      engine_mount_point: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/kv"
      auth_method: approle
      mount_point: "{{ vault_nodes_approle }}"
      role_id: "{{ vault_role_id }}"
      secret_id: "{{ vault_secret_id }}"
      token_validate: false
    register: get_ca_key
    ignore_errors: true

  - ansible.builtin.set_fact:
      k8s_ca_key_avail_in_vault: >
        {{ (get_ca_key is success)
           and ((get_ca_key.data.data.private_key | default("")) != "") }}

  - name: "Print availability of CA key in Vault"
    debug:
      msg: >
        Kubernetes cluster root CA key is
        {{ k8s_ca_key_avail_in_vault | ternary('', 'un') }}available in Vault
        and should be
        {{ k8s_controller_manager_enable_signing_requests | ternary('', 'un') }}available
        because Kubernetes certificate signing is
        {{ k8s_controller_manager_enable_signing_requests | ternary('en', 'dis') }}abled.

  - name: "Ensure CA key is unavailable in Vault when signing requests are disabled"
    when: not k8s_controller_manager_enable_signing_requests
    block:
    # NOTE: We don't require explicit consent here as the deletion is not
    #       actually destructive. Users are able to undelete.
    - name: "Ensure CA key is unavailable in Vault: Delete key"
      no_log: true  # prevent private key from being exposed
      # NOTE: We only attempt to delete when there is actually something to delete
      #       so that a Vault root token only needs to be present in this case.
      when: k8s_ca_key_avail_in_vault
      community.hashi_vault.vault_kv2_delete:
        path: k8s-pki/cluster-root-ca
        engine_mount_point: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/kv"
        # NOTE: Because the Vault role 'k8s-control-plane' is not allowed to
        #       delete the CA key in Vault, we need to use the priviledges of
        #       the caller. Currently it must insert a Vault root token here.
        auth_method: "{{ vault_caller_auth_method }}"
        mount_point: "{{ vault_caller_auth_mount_point }}"
        token: "{{ vault_caller_token }}"
        role_id: "{{ vault_caller_role_id }}"
        secret_id: "{{ vault_caller_secret_id }}"
        token_validate: false
    rescue:
    - name: "Ensure CA key is unavailable in Vault: Hint failure reason"
      ansible.builtin.fail:
        msg: |
          Vault root token required!
          Disabling Kubernetes signing requests entails the removal of the
          Kubernetes cluster CA key from Vault's key-value store.
          This is done by the orchestrator and requires a Vault root token.

  - name: "Ensure CA key is available in Vault when signing requests are enabled"
    when: k8s_controller_manager_enable_signing_requests
    block:
    - name: "Require CA key to be available in Vault"
      ansible.builtin.fail:  # noop task, lookup is already done above
      # NOTE: ansible.builtin.meta: noop supports no conditionals
      failed_when: not k8s_ca_key_avail_in_vault
    rescue:
    - name: "Require CA key to be available in Vault: Hint failure reason"
      ansible.builtin.fail:
        msg: |
          Manual intervention required!
          Enabling Kubernetes signing requests requires the CA key to be
          available in Vault's key-value store at 'k8s-pki/cluster-root-ca'.
          This is done either by running the rotate-root-ca action or manual
          upload of an externally available key.

- name: Obtain CAs
  include_tasks: obtain-cas.yaml

- name: Obtain kubeconfigs
  include_tasks: obtain-kubeconfigs.yaml

- name: Obtain certificates
  include_tasks: obtain-certs.yaml

- name: Delete secret data which is now in Vault
  become: true
  ansible.builtin.file:
    state: absent
    path: /etc/kubernetes/pki/{{ item }}
  loop:
  - front-proxy-ca.key
  - etcd/ca.key

- name: Ensure Kubernetes ca.key
  become: true
  ansible.builtin.file:
    state: "{{ k8s_controller_manager_enable_signing_requests | ternary('file', 'absent') }}"
    path: /etc/kubernetes/pki/ca.key
    mode: 0400

- name: Ensure kube-controller-manager client cert flags are (un)set
  become: true
  when: local_kube_apiserver_status is not failed
  block:
  - name: Ensure cluster-signing-cert-file is (un)set
    ansible.builtin.replace:
      path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      regexp: '^(\s*-\s*--cluster-signing-cert-file=).*'
      replace: "{{ k8s_controller_manager_enable_signing_requests | ternary('\\1/etc/kubernetes/pki/ca.crt', '\\1') }}"

  - name: Ensure --cluster-signing-key-file is (un)set
    ansible.builtin.replace:
      path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      regexp: '^(\s*-\s*--cluster-signing-key-file=).*'
      replace: "{{ k8s_controller_manager_enable_signing_requests | ternary('\\1/etc/kubernetes/pki/ca.key', '\\1') }}"

- name: Ensure that /var/lib/etcd exists
  become: true
  ansible.builtin.file:
    path: /var/lib/etcd
    state: directory
    mode: u=rwx,go-rwx

- name: Spawn K8s cluster with 'kubeadm init'
  when: "local_kube_apiserver_status is failed and lb_kube_apiserver_status is failed"
  tags:
  - spawn
  become: true
  block:
  - name: Pull config images used by kubeadm from registry.k8s.io  # noqa no-changed-when
    ansible.builtin.command: kubeadm config images pull --kubernetes-version {{ k8s_version }}

  - name: Create kubeadm-init-config.yaml
    vars:
      gateway: "{{ groups['gateways'] | first }}"
    ansible.builtin.template:
      src: kubeadm-init-config.yaml.j2
      dest: /tmp/kubeadm-init-config.yaml
      owner: root
      group: root
      mode: 0600

  - name: Run kubeadm init  # noqa no-changed-when
    # XXX: ansible command module does not support setting the umask
    ansible.builtin.shell: umask 077 && kubeadm init --node-name={{ inventory_hostname | quote }} --config=/tmp/kubeadm-init-config.yaml

  - name: Remove kubeadm-init-config.yaml
    ansible.builtin.file:
      path: /tmp/kubeadm-init-config.yaml
      state: absent

  - name: Configure bridge-nf-call-iptables
    ansible.posix.sysctl:
      name: net.bridge.bridge-nf-call-iptables
      value: 1
      state: present

- name: Join the K8s control plane
  when: "local_kube_apiserver_status is failed and not lb_kube_apiserver_status is failed"
  become: true
  environment:
    ANSIBLE_HASHI_VAULT_URL: "{{ lookup('env', 'VAULT_ADDR') }}"
    ANSIBLE_HASHI_VAULT_CA_CERT: "{{ lookup('env', 'VAULT_CACERT') }}"
  block:
  - name: Get certificate information
    community.crypto.x509_certificate_info:
      path: "/etc/kubernetes/pki/ca.crt"

  - name: Create kubeadm-join-config.yaml
    ansible.builtin.template:
      src: kubeadm-join-config.yaml.j2
      dest: /tmp/kubeadm-join-config.yaml
      owner: root
      group: root
      mode: 0600

  - name: Join the existing cluster as another control plane node  # noqa no-changed-when
    # XXX: ansible command module does not support setting the umask
    ansible.builtin.shell: "umask 077 && kubeadm join --config=/tmp/kubeadm-join-config.yaml"

  - name: Remove kubeadm-join-config.yaml
    ansible.builtin.file:
      path: /tmp/kubeadm-join-config.yaml
      state: absent

- name: Login to the Kubernetes cluster as orchestrator
  ansible.builtin.include_role:
    name: k8s-login

- name: Provide CA certificates as ConfigMap
  include_tasks: provide-ca-as-cm.yaml
...
