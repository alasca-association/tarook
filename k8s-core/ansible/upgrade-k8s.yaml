---
- name: Initialize node connection
  import_playbook: "connect-to-nodes.yaml"
  vars:
    target_hosts: k8s_nodes

- name: Renew PKI (CP)
  hosts: masters
  gather_facts: true
  vars_files:
    - vars/etc.yaml
    - vars/retries.yaml
    - vars/vault-config.yaml
  serial:
    - 1
    - "100%"
  tasks:
    - name: Obtain CAs (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-cas.yaml
      tags:
        - renew-pki
    - name: Obtain kubeconfigs (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-kubeconfigs.yaml
      tags:
        - renew-pki
    - name: Obtain certificates (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-certs.yaml
      tags:
        - renew-pki
    - name: Provide CA certificates as ConfigMap
      include_role:
        name: k8s-master
        tasks_from: provide-ca-as-cm.yaml
      tags:
        - renew-pki

- name: Renew PKI (Worker)
  hosts: workers
  gather_facts: true
  vars_files:
    - vars/etc.yaml
    - vars/retries.yaml
    - vars/vault-config.yaml
  tasks:
    - name: Obtain CAs (Worker)
      include_role:
        name: k8s-worker
        tasks_from: obtain-cas.yaml
      tags:
        - renew-pki
    - name: Obtain kubeconfigs (Worker)
      include_role:
        name: k8s-worker
        tasks_from: obtain-kubeconfigs.yaml
      tags:
        - renew-pki

- name: Upgrade all but first master nodes
  hosts: masters[1:]
  gather_facts: true
  serial: 1
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - kubeadm-other-masters
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-patches
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"

- name: Upgrade the first master node
  hosts: masters[0]
  gather_facts: true
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/etc.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - kubeadm-first-master
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-patches
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-apply
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"

- name: Upgrade kubelet on master nodes
  hosts: masters
  gather_facts: true
  any_errors_fatal: true
  tags:
    - kubelet-masters
  serial: 1
  vars_files:
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  roles:
    - role: kubeadm-upgrade-kubelet
      when: "not (k8s_upgrade_done | default(False))"
      # we do not want to customize kubelet on control-plane nodes
      vars:
        k8s_kubelet_disable_customizations: true

- name: Upgrade the worker nodes
  hosts: workers
  gather_facts: true
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - workers
  serial: 1
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubelet
      when: "not (k8s_upgrade_done | default(False))"
  # The nvidia-device-plugin marks a GPU as unhealthy
  # on systemctl daemon-reloads + kubelet restarts,
  # but the Pod does not fail. We have to manually restart the Pod.
  tasks:
    - name: Restart nvidia-device-plugin Pod on node
      when:
        - "not (k8s_upgrade_done | default(False))"
      ansible.builtin.include_role:
        name: nvidia-device-plugin
        tasks_from: restart_ndp_pod.yaml
    - name: Uncordon the node
      ansible.builtin.include_role:
        name: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"

# Drop me with Kubernetes v1.28
# This is needed to replace the existing with a non super admin kubeconfig
# after the upgrade to Kubernetes v1.29 happened
- name: Generate non super admin kubeconfig
  hosts: masters
  vars_files:
    - vars/vault-config.yaml
  tasks:
    - name: Generate non super admin kubeconfig
      when:
        - k8s_version is version('1.29', '<')
        - next_k8s_version is version('1.29', '>=')
      include_role:
        name: k8s-master
        tasks_from: mkkubeconfig.yaml
      vars:
        vault_role_id: "{{ vault_node_role_id }}"
        vault_secret_id: "{{ vault_node_secret_id }}"
        kubeconfig_title: admin
        kubeconfig_destination: /etc/kubernetes/admin.conf
        kubeconfig_role: cluster-admins_admin
        kubeconfig_user: kubernetes-admin
        force_renewal: true
        kubeconfig_api_server_url: "https://{{ networking_fixed_ip }}:{{ k8s_apiserver_frontend_port }}"
...
