---
- name: Initialize node connection
  import_playbook: "connect-to-nodes.yaml"
  vars:
    target_hosts: k8s_nodes

- name: Renew PKI (CP)
  hosts: masters
  gather_facts: true
  vars_files:
    - vars/etc.yaml
    - vars/retries.yaml
    - vars/vault-config.yaml
  serial:
    - 1
    - "100%"
  tasks:
    - name: Obtain CAs (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-cas.yaml
      tags:
        - renew-pki
    - name: Obtain kubeconfigs (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-kubeconfigs.yaml
      tags:
        - renew-pki
    - name: Obtain certificates (CP)
      include_role:
        name: k8s-master
        tasks_from: obtain-certs.yaml
      tags:
        - renew-pki
    - name: Provide CA certificates as ConfigMap
      include_role:
        name: k8s-master
        tasks_from: provide-ca-as-cm.yaml
      tags:
        - renew-pki

- name: Renew PKI (Worker)
  hosts: workers
  gather_facts: true
  vars_files:
    - vars/etc.yaml
    - vars/retries.yaml
    - vars/vault-config.yaml
  tasks:
    - name: Obtain CAs (Worker)
      include_role:
        name: k8s-worker
        tasks_from: obtain-cas.yaml
      tags:
        - renew-pki
    - name: Obtain kubeconfigs (Worker)
      include_role:
        name: k8s-worker
        tasks_from: obtain-kubeconfigs.yaml
      tags:
        - renew-pki

- name: Upgrade the first master node
  hosts: masters[0]
  gather_facts: true
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/etc.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - kubeadm-first-master
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-patches
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-apply
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"

- name: Upgrade the residual master nodes
  hosts: masters[1:]
  gather_facts: true
  serial: 1
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - kubeadm-other-masters
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-patches
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"

- name: Upgrade kubelet on master nodes
  hosts: masters
  gather_facts: true
  any_errors_fatal: true
  tags:
    - kubelet-masters
  serial: 1
  vars_files:
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  roles:
    - role: kubeadm-upgrade-kubelet
      when: "not (k8s_upgrade_done | default(False))"
      # we do not want to customize kubelet on control-plane nodes
      vars:
        k8s_kubelet_disable_customizations: true

- name: Upgrade the worker nodes
  hosts: workers
  gather_facts: true
  any_errors_fatal: true
  vars_files:
    - vars/disruption.yaml
    - vars/retries.yaml
    - vars/auto_generated_preamble.yaml
  tags:
    - workers
  serial: 1
  roles:
    - role: cluster-health-verification
      delegate_to: "{{ groups['orchestrator'] | first }}"
      when: not (k8s_skip_upgrade_checks | bool) and not (k8s_upgrade_done | default(False))
    - role: kubeadm-drain-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: update-system
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubeadm
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-node
      when: "not (k8s_upgrade_done | default(False))"
    - role: kubeadm-upgrade-kubelet
      when: "not (k8s_upgrade_done | default(False))"
  # The nvidia-device-plugin marks a GPU as unhealthy
  # on systemctl daemon-reloads + kubelet restarts,
  # but the Pod does not fail. We have to manually restart the Pod.
  tasks:
    - name: Restart nvidia-device-plugin Pod on node
      when:
        - "not (k8s_upgrade_done | default(False))"
      ansible.builtin.include_role:
        name: nvidia-device-plugin
        tasks_from: restart_ndp_pod.yaml
    - name: Uncordon the node
      ansible.builtin.include_role:
        name: kubeadm-uncordon-node
      when: "not (k8s_upgrade_done | default(False))"
...
