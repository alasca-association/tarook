[ansible.02_trampoline.group_vars.gateways]
lb_ports = [
  30060
]
cah_users_include_users = ["<user>", "<user>"]

# IP address range to use for WireGuard clients. Must be set to a CIDR and must
# not conflict with the terraform.subnet_cidr.
#
# Should be chosen uniquely for all clusters of a customer at the very least
# so that they can use all of their clusters at the same time without having
# to tear down tunnels.
wg_ip_cidr = "172.30.153.64/26"
wg_ip_gw = "172.30.153.65/26"

# To add WireGuard keys for customers, create blocks like the following:
#
#[[ansible.02_trampoline.group_vars.gateways.wg_peers]]
#pub_key = "test1"
#ident = "testkunde1"
#
# You can add as many of them as you want. toml_helper.py will auto-allocate IP
# addresses from the wg_ip_cidr.

[ansible.02_trampoline.host_vars.managed-k8s-gw-az1]
vrrp_priority = 150

[ansible.02_trampoline.host_vars.managed-k8s-gw-az2]
vrrp_priority = 100

[ansible.02_trampoline.host_vars.managed-k8s-gw-az3]
vrrp_priority = 80

[ansible.03_final.group_vars.all]
# KUBERNETES CLUSTER CONFIGURATION
#
# Kubernetes version. Currently, we only support 1.17.x.
k8s_version = "1.17.3"

# Pick a networking plugin:
#
# - kube-router: High-performance, low-overhead implementation with support
#   for NetworkPolicy objects
# - flannel: Historical default option. Do not use for new clusters. It is
#   still the default (from the ansible side) to not break existing clusters.
k8s_network_plugin = "kube-router"

# When upgrading to kube-router, configuration on how to handle the transition
# is required. Note that all pods MUST be restarted (otherwise they’ll have no
# connectivity after the restart).
#
# If set to true, ansible will restart all DaemonSets, StatefulSets and
# Deployments in all namespaces via `kubectl rollout restart`. If set to false,
# only our managed namespaces (rook-ceph, monitoring and kube-system) are
# restarted, which means that the customer will have to do their own restarts.
#k8s_network_plugin_switch_restart_all_namespaces=true
#
# This can be used to change the set of namespaces where restarts are
# automatically carried out if k8s_network_plugin_switch_restart_all_namespaces
# is set to false. Generally, you’ll not want to change this.
#
#k8s_network_plugin_switch_restart_namespaces=[...]

# To enable our LBaaS service, un-comment the following options and fill in a
# unique, random, base64-encoded secret in place of `...`.
#
# To generate such a secret, you can use the following command:
#
# $ dd if=/dev/urandom bs=16 count=1 status=none | base64
#
#ch_k8s_lbaas = true
#ch_k8s_lbaas_shared_secret = "..."
#ch_k8s_lbaas_version = "0.3.0"

rook = true
rook_nosds = 3
rook_osd_volume_size = "90Gi"
rook_toolbox = true
rook_fs = true

# Rook pod memory limits
#
# The default values are the *absolute minimum* values required by rook. Going
# below these numbers will make rook refuse to even create the pods. See also:
# https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#cluster-wide-resources-configuration-settings

# Memory limit for mon Pods
#
#rook_mon_memory_limit = "1Gi"
#rook_mon_memory_request = "{{ rook_mon_memory_limit }}"
#rook_mon_cpu_limit = "500m"
#rook_mon_cpu_request = "100m"

# Resource limits for OSD pods
#
# Note that these are chosen so that the OSD pods end up in the Guaranteed QoS
# class.
#
#rook_osd_memory_limit = "2Gi"
#rook_osd_memory_request = "{{ rook_osd_memory_limit }}"
#rook_osd_cpu_limit = "500m"
#rook_osd_cpu_request = "{{ rook_osd_cpu_limit }}"

# Memory limit for mgr Pods
#
#rook_mgr_memory_limit = "512Mi"
#rook_mgr_memory_request = "{{ rook_mgr_memory_limit }}"
#rook_mgr_cpu_limit = "500m"
#rook_mgr_cpu_request = "100m"

# Memory limit for MDS / CephFS Pods
#
#rook_mds_memory_limit = "4Gi"
#rook_mds_memory_request = "{{ rook_mds_memory_limit }}"
#rook_mds_cpu_limit = "1"
#rook_mds_cpu_request = "{{ rook_mds_cpu_limit }}"

cah_users_include_users = ["<user>", "<user>"]

[ansible.03_final.host_vars.managed-k8s-gw-az1]
vrrp_priority = 150

[ansible.03_final.host_vars.managed-k8s-gw-az2]
vrrp_priority = 100

[ansible.03_final.host_vars.managed-k8s-gw-az3]
vrrp_priority = 80

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-0"
name = "worker0"

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-1"
name = "worker1"

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-2"
name = "worker2"

[terraform]
subnet_cidr = "172.30.154.0/24"
workers = 3
