[ansible.02_trampoline.group_vars.gateways]
# lb_ports is a list of ports that are exposed by HAProxy on the gateway nodes and forwarded
# to NodePorts in the k8s cluster. This poor man's load-balancing / exposing of services
# has been superseded by ch-k8s-lbaas. For legacy reasons and because it's useful under
# certain circumstances it is kept inside the repository.
# The NodePorts are either literally exposed by HAProxy or can be mapped to other ports.
# Short form:
#lb_ports = [30060]
# Explicit form:
#lb_ports = [{external=80,nodeport=30080}]

# Include and exclude C&H users from rollout
# C&H users refers to items in the ch-users-databag repository
# For additional information refer the ch-role-users repository
# The users have to be provided as List<String>
# Possible Configurations:
# - cah_users_roll_out_users_from
#   - include specific user group
#   - Default: ["opsi", "it-operations", "head"]
# - cah_users_exclude_users_from
#   - exclude specific user group
#   - Default: ["students", "service"]
# - cah_users_include_users
#   - include specific user(s)
#   - Default: []
# - cah_users_exclude_users
#   - exclude specific user(s)
#   - Default: ["deployer"]
# Example Usage:
# cah_users_include_users = ["<user>", "<user>"]

# IP address range to use for WireGuard clients. Must be set to a CIDR and must
# not conflict with the terraform.subnet_cidr.
#
# Should be chosen uniquely for all clusters of a customer at the very least
# so that they can use all of their clusters at the same time without having
# to tear down tunnels.
wg_ip_cidr = "172.30.153.64/26"
wg_ip_gw = "172.30.153.65/26"

# IPSec tunnels
ipsec_enabled = false

# Must be a list of parent SA proposals to offer to the client.
# Must be explicitly set if ipsec_enabled is set to true.
#ipsec_proposals =

# Must be a list of ESP proposals to offer to the client.
# Defaults to ipsec_proposals
#ipsec_esp_proposals = "{{ ipsec_proposals }}"

# List of CIDRs to route to the peer. If not set, only dynamic IP
# assignments will be routed.
# Defaults to null to disable.
#ipsec_peer_networks = []

# List of CIDRs to offer to the peer. Defaults to a list with only the
# `subnet_cidr` in it.
#ipsec_local_networks = ["{{ subnet_cidr }}"]

# Pool to source virtual IP addresses from. Those are the IP addresses assigned
# to clients which do not have remote networks. Disabled by default.
#ipsec_virtual_subnet_pool = false

# List of addresses to accept as remote. When initiating, the first single IP
# address is used.
# Default is false, disabling the filter altogether.
#ipsec_remote_addrs = false

# The PSK for EAP. Must be set.
#ipsec_eap_psk =

## Wireguard-based site-to-site tunnel
# If enabled, configure site to site tunnel (defaults to false)
#wg_s2s_enabled = true

# Subnet of the wireguard "transfer net" between the two endpoints
#wg_s2s_transfer_subnet = "172.30.18.2/31"

# IP which is assigned to our VRRP master in transfer network
#wg_s2s_ip = "172.30.18.2"

# IP which is assigned to our VRRP master in transfer network
#wg_s2s_peer_ip = "172.30.18.3"

# Port on which wireguard listens
#wg_s2s_port = "16000"

# Public wireguard key of the peer
#wg_s2s_peer_pub_key = "7CuC/cSw1US+nilx0ihoA1qb2DsQI0QV2RBuLE8cnhk="

# Endpoint under which the peer can be reached
#wg_s2s_peer_public_endpoint = "<public-IP-of-your-peer>:16000"

# BGP AS IDs for both parties (should differ, unless iBGP is wanted)
#wg_s2s_bgp_as = "65010"
#wg_s2s_peer_bgp_as = "65009"



# To add WireGuard keys for customers, create blocks like the following:
#
#[[ansible.02_trampoline.group_vars.gateways.wg_peers]]
#pub_key = "test1"
#ident = "testkunde1"
#
# You can add as many of them as you want. toml_helper.py will auto-allocate IP
# addresses from the wg_ip_cidr.

[ansible.02_trampoline.host_vars.managed-k8s-gw-az1]
vrrp_priority = 150

[ansible.02_trampoline.host_vars.managed-k8s-gw-az2]
vrrp_priority = 100

[ansible.02_trampoline.host_vars.managed-k8s-gw-az3]
vrrp_priority = 80

[ansible.03_final.group_vars.all]
# KUBERNETES CLUSTER CONFIGURATION
#
# Kubernetes version. Currently, we support 1.17.* and 1.18.*.
k8s_version = "1.18.18"

# Pick a networking plugin:
#
# - kube-router: High-performance, low-overhead implementation with support
#   for NetworkPolicy objects
# - flannel: Historical default option. Do not use for new clusters. It is
#   still the default (from the ansible side) to not break existing clusters.
k8s_network_plugin = "kube-router"

# When upgrading to kube-router, configuration on how to handle the transition
# is required. Note that all pods MUST be restarted (otherwise they’ll have no
# connectivity after the restart).
#
# If set to true, ansible will restart all DaemonSets, StatefulSets and
# Deployments in all namespaces via `kubectl rollout restart`. If set to false,
# only our managed namespaces (rook-ceph, monitoring and kube-system) are
# restarted, which means that the customer will have to do their own restarts.
#k8s_network_plugin_switch_restart_all_namespaces=true
#
# This can be used to change the set of namespaces where restarts are
# automatically carried out if k8s_network_plugin_switch_restart_all_namespaces
# is set to false. Generally, you’ll not want to change this.
#
#k8s_network_plugin_switch_restart_namespaces=[...]

# NETWORK CONFIGURATION

# This is the subnet used by Kubernetes for Pods. Subnets will be delegated
# automatically to each node.
#k8s_network_pod_subnet = "10.244.0.0/16"

# This is the subnet used by Kubernetes for Services.
#k8s_network_service_subnet = "10.96.0.0/12"

# To enable our LBaaS service, un-comment the following options and fill in a
# unique, random, base64-encoded secret in place of `...`.
#
# To generate such a secret, you can use the following command:
#
# $ dd if=/dev/urandom bs=16 count=1 status=none | base64
#
#ch_k8s_lbaas = true
#ch_k8s_lbaas_shared_secret = "..."
#ch_k8s_lbaas_version = "0.3.3"

# ROOK-BASED CEPH STORAGE CONFIGURATION
#
# By default, rook is disabled. Many clusters will want to use it though, so
# you should enable it here if you want. It requires extra options which need
# to be chosen with care, which is why the default is `false`.
#
# Enable rook deployment
#rook = true

# Placement controls
#
# These controls allow to control the placement of rook pods. The `taint` option
# controls which additional taint the rook pods will tolerate. Note that the
# only supported key is "{{ managed_k8s_control_plane_key }}", but the value
# can be chosen freely to allow to separate e.g. monitoring and ceph on
# different workers.
#
# The `label` option constraints pods to run *only* on nodes with the specific
# label. If no nodes have that label, the cluster will not start.
#
# It is recommended to set both taint and label to the same thing.
#
#rook_placement_taint = { key = "{{ managed_k8s_control_plane_key }}", value = "meta" }
#
#rook_placement_label = { key = "{{ managed_k8s_control_plane_key }}", value = "meta" }

# Number of OSDs to run. This should be equal to the number of storage meta
# workers you use.
#rook_nosds = 3

# The size of the storage backing each OSD.
#rook_osd_volume_size = "90Gi"

# Enable the rook toolbox, which is a pod with ceph tools installed to
# introspect the cluster state.
#rook_toolbox = true

# Enable the CephFS shaerd filesystem.
#rook_fs = true

# Rook pod memory limits
#
# The default values are the *absolute minimum* values required by rook. Going
# below these numbers will make rook refuse to even create the pods. See also:
# https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#cluster-wide-resources-configuration-settings

# Memory limit for mon Pods
#
#rook_mon_memory_limit = "1Gi"
#rook_mon_memory_request = "{{ rook_mon_memory_limit }}"
#rook_mon_cpu_limit = "500m"
#rook_mon_cpu_request = "100m"

# Resource limits for OSD pods
#
# Note that these are chosen so that the OSD pods end up in the Guaranteed QoS
# class.
#
#rook_osd_memory_limit = "2Gi"
#rook_osd_memory_request = "{{ rook_osd_memory_limit }}"
#rook_osd_cpu_limit = "500m"
#rook_osd_cpu_request = "{{ rook_osd_cpu_limit }}"

# Memory limit for mgr Pods
#
#rook_mgr_memory_limit = "512Mi"
#rook_mgr_memory_request = "{{ rook_mgr_memory_limit }}"
#rook_mgr_cpu_limit = "500m"
#rook_mgr_cpu_request = "100m"

# Memory limit for MDS / CephFS Pods
#
#rook_mds_memory_limit = "4Gi"
#rook_mds_memory_request = "{{ rook_mds_memory_limit }}"
#rook_mds_cpu_limit = "1"
#rook_mds_cpu_request = "{{ rook_mds_cpu_limit }}"

# MONITORING
# Enable monitoring (enabled by default)
#monitoring = true

# Enable use of Thanos (enabled by default)
#monitoring_use_thanos = true

# Thanos uses local storage to keep a copy of the metadata from the object store
# for faster access. The size and storage class for that volume can be
# configured:
#
#monitoring_thanos_metadata_volume_size="10Gi"
#monitoring_thanos_metadata_volume_storage_class="rook-ceph-data"

# By default, the monitoring will capture all namespaces. If this is not
# desired, the following switch can be turned off. In that case, only the
# kube-system, monitoring and rook namespaces are scraped by Prometheus.
#
#monitoring_prometheus_monitor_all_namespaces=true

# Placement controls
#
# These controls allow to control the placement of monitoring pods. The `taint`
# option controls which additional taint the monitoring pods will tolerate. Note
# that the only supported key is "{{ managed_k8s_control_plane_key }}", but the
# value can be chosen freely to allow to separate e.g. monitoring and ceph on
# different workers.
#
# The `label` option constraints pods to run *only* on nodes with the specific
# label. If no nodes have that label, the cluster will not start.
#
# It is recommended to set both taint and label to the same thing.
#
#monitoring_placement_taint = { key = "{{ managed_k8s_control_plane_key }}", value = "meta" }
#
#monitoring_placement_label = { key = "{{ managed_k8s_control_plane_key }}", value = "meta" }
#
# To use the same taint and label controls as rook, set them in the following
# way:
#monitoring_placement_taint = "{{ rook_placement_taint }}"
#monitoring_placement_label = "{{ rook_placement_label }}"

# Monitoring pod resource limits
#
# The following limits are applied to the respective pods.
#
# Note that the Prometheus limits are chosen fairly conservatively and may need
# tuning for larger and smaller clusters.
#
# By default, we prefer to set limits in such a way that the Pods end up in the
# Guaranteed QoS class (i.e. both CPU and Memory limits and requests set to the
# same value).
#
#monitoring_alertmanager_memory_limit = "256Mi"
#monitoring_alertmanager_memory_request = "{{ monitoring_alertmanager_memory_limit }}"
#monitoring_alertmanager_cpu_limit = "100m"
#monitoring_alertmanager_cpu_request = "{{ monitoring_alertmanager_cpu_limit }}"
#
#monitoring_prometheus_memory_limit = "3Gi"
#monitoring_prometheus_memory_request = "{{ monitoring_prometheus_memory_limit }}"
#monitoring_prometheus_cpu_limit = "1"
#monitoring_prometheus_cpu_request = "{{ monitoring_prometheus_cpu_limit }}"
#
#monitoring_grafana_memory_limit = "512Mi"
#monitoring_grafana_memory_request = "256Mi"
#monitoring_grafana_cpu_limit = "500m"
#monitoring_grafana_cpu_request = "100m"
#
#monitoring_kube_state_metrics_memory_limit = "128Mi"
#monitoring_kube_state_metrics_memory_request = "50Mi"
#monitoring_kube_state_metrics_cpu_limit = "50m"
#monitoring_kube_state_metrics_cpu_request = "20m"
#
#monitoring_thanos_sidecar_memory_limit = "256Mi"
#monitoring_thanos_sidecar_memory_request = "{{ monitoring_thanos_sidecar_memory_limit }}"
#monitoring_thanos_sidecar_cpu_limit = "500m"
#monitoring_thanos_sidecar_cpu_request = "{{ monitoring_thanos_sidecar_cpu_limit }}"
#
# Note that the default limits for Thanos are quite experimental. We’re still
# gathering experience here. Please see also
# ansible/roles/k8s-monitoring/defaults/main.yaml for more discussion on the
# defaults.
#
# Instead of ad-hoc changing limits in your cluster, seeking discussion with
# the Flyingdutchman/CID team for a change of defaults is to be preferred.
#
#monitoring_thanos_query_memory_limit = "786Mi"
#monitoring_thanos_query_memory_request = "128Mi"
#monitoring_thanos_query_cpu_limit = "1"
#monitoring_thanos_query_cpu_request = "100m"
#
#monitoring_thanos_store_memory_limit = "2Gi"
#monitoring_thanos_store_memory_request = "256Mi"
#monitoring_thanos_store_cpu_limit = "500m"
#monitoring_thanos_store_cpu_request = "100m"

# MISC FEATURES

# Install wireguard on all workers (without setting up any server-side stuff)
# so that in can be used from within Pods.
#wireguard_on_workers = true

# Uncomment if this cluster contains a worker with GPU access so that the driver
# and surrounding framework is deployed.
#is_gpu_cluster = true

# Enforce the use of pod security policies inside the cluster.
#k8s_use_podsecuritypolicies = true

# Name of the internal OpenStack network. This field becomes important if a VM is
# attached to two networks but the controller-manager should only pick up one. If 
# you don't understand the purpose of this field, there's a very high chance you
# won't need to touch it/uncomment it.
# Note: This network name isn't fetched automagically (by terraform) on purpose
# because there might be situations where the CCM should not pick the managed network.
#openstack_network_name = "managed-k8s-network"

# Include and exclude C&H users from rollout
# C&H users refers to items in the ch-users-databag repository
# For additional information refer the ch-role-users repository
# The users have to be provided as List<String>
# Possible Configurations:
# - cah_users_roll_out_users_from
#   - include specific user group
#   - Default: ["opsi", "it-operations", "head"]
# - cah_users_exclude_users_from
#   - exclude specific user group
#   - Default: ["students", "service"]
# - cah_users_include_users
#   - include specific user(s)
#   - Default: []
# - cah_users_exclude_users
#   - exclude specific user(s)
#   - Default: ["deployer"]
# Example Usage:
# cah_users_include_users = ["<user>", "<user>"]

[ansible.03_final.host_vars.managed-k8s-gw-az1]
vrrp_priority = 150

[ansible.03_final.host_vars.managed-k8s-gw-az2]
vrrp_priority = 100

[ansible.03_final.host_vars.managed-k8s-gw-az3]
vrrp_priority = 80

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-0"
name = "worker0"

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-1"
name = "worker1"

[[ansible.03_final.group_vars.all.test_worker_nodes]]
worker = "managed-k8s-worker-2"
name = "worker2"

# A special section that does not directly map to an ansible-stage.
[secrets]
# Set this field to `false` if the "company" users should not be rolled out.
# passwordstore_rollout_company_users = true

# Configure Additional GPG-IDs that should have access to the cluster-repo specific passwordstore.
# If you're not member of the "company-wide" list, e.g., because you're a student you must add yourself here.
# yannic.ahrens@cloudandheat.com serves as an example here because the author is incredibly vain and likes
# to see his name written down everywhere.
# Parameters:
#   - 'ident': easy to read identification string, not used anywhere yet
#   - 'gpg_id': ID of your public GPG key, ideally in long-form

[[secrets.passwordstore_additional_users]]
ident = "yannic.ahrens@cloudandheat.com"
gpg_id = "68AA582E81AD111C127F01273370EBE296354805"

[terraform]
subnet_cidr = "172.30.154.0/24"
workers = 3

# If true, create block volume for each instance and boot from there.
# Equivalent to `openstack server create --boot-from-volume […]. (`false` by default)
#create_root_disk_on_volume = true

# Volume type that is used if `create_root_disk_on_volume` is true. ("three_times_replicated" by default)
#root_disk_volume_type = "three_times_replicated"
