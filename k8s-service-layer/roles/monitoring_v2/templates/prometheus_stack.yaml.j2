## Labels to apply to all resources
##
{% if monitoring_common_labels %}
commonLabels:
{% for label_key, label_value in monitoring_common_labels.items() %}
  {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}

priorityClassName: "system-cluster-critical"

##
defaultRules:
  create: true
  rules:
    etcd: false # disabled for now
    kubeApiserver: false # https://github.com/prometheus-community/helm-charts/issues/1283

##
global:
  rbac:
    create: true

##
alertmanager:
  enabled: true
  alertmanagerSpec:
    priorityClassName: "system-cluster-critical"
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
{% if monitoring_scheduling_key %}
  tolerations:
    - key: "{{ monitoring_scheduling_key }}"
      operator: Exists
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: "{{ monitoring_scheduling_key }}"
              operator: Exists
{% endif %}

##
grafana:
  priorityClassName: "system-cluster-critical"
  enabled: {{ monitoring_use_grafana | bool }}
  persistence:
    enabled: {{ monitoring_grafana_persistent_storage_class | ternary(true,false) }}
    storageClassName: "{{ monitoring_grafana_persistent_storage_class }}"
  admin:
    existingSecret: {{ monitoring_grafana_admin_secret_name }}
    userKey: admin-user
    passwordKey: admin-password
{% if monitoring_scheduling_key %}
  tolerations:
    - key: "{{ monitoring_scheduling_key }}"
      operator: Exists
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "{{ monitoring_scheduling_key }}"
                operator: Exists
{% endif %}
{% if monitoring_grafana_persistent_storage_class %}
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: "app.kubernetes.io/name"
            operator: In
            values:
            - "grafana"
        topologyKey: "kubernetes.io/hostname"
{% endif %}
  datasources:
    datasources.yaml:
      apiVersion: 1
{% if monitoring_use_thanos %}
      datasources:
        - name: thanos
          type: prometheus
          access: proxy
          orgId: 1
          url: "http://thanos-query.{{ monitoring_namespace }}.svc:9090"
          version: 1
          editable: false
{% endif %}
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      searchNamespace: "ALL"
      folderAnnotation: customer-dashboards
      provider:
        foldersFromFilesStructure: true
  serviceMonitor:
    enabled: true
{% if monitoring_common_labels %}
    labels:
{% for label_key, label_value in monitoring_common_labels.items() %}
      {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}
    # https://github.com/prometheus-community/helm-charts/issues/1776
    interval: "30s"
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
  dashboards:
    managed:
      dashboard-calico:
        gnetId: 12175
        revision: 5
        datasource: Prometheus
{% if k8s_storage_rook_enabled %}
      dashboard-ceph-cluster:
        gnetId: 2842
        revision: 14
        datasource: Prometheus
      dashboard-ceph-osd-single:
        gnetId: 5336
        revision: 5
        datasource: Prometheus
      dashboard-ceph-pools:
        gnetId: 5342
        revision: 5
        datasource: Prometheus
{% endif %}
  dashboardProviders:
    managed-dashboard-provider.yaml:
      apiVersion: 1
      providers:
        - name: 'managed-dashboards'
          folder: 'managed-dashboards'
          options:
            path: /var/lib/grafana/dashboards/managed

##
kubeApiServer:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels:
          - __meta_kubernetes_namespace
          - __meta_kubernetes_service_name
          - __meta_kubernetes_endpoint_port_name
        action: keep
        regex: default;kubernetes;https
      - targetLabel: __address__
        replacement: kubernetes.default.svc:443
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
kubelet:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
        action: replace

##
kubeControllerManager:
  enabled: true
  service:
    port: 10257
    targetPort: 10257
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
coreDNS:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
kubeEtcd:
  enabled: true
  service:
    enabled: true
    port: 2381
    targetPort: 12381
    selector:
      app.kubernetes.io/name: etcd-proxy-metrics
  serviceMonitor:
    enabled: true
    scheme: https
    insecureSkipVerify: false
    caFile: /etc/prometheus/secrets/etcd-metrics-proxy/server.crt
    certFile: /etc/prometheus/secrets/etcd-metrics-proxy/client.crt
    keyFile: /etc/prometheus/secrets/etcd-metrics-proxy/client.key

##
kubeScheduler:
  enabled: true
  service:
    enabled: true
    port: 10259
    targetPort: 10259
  serviceMonitor:
    enabled: true
    https: true
    insecureSkipVerify: true
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
kubeProxy:
  enabled: {{ (k8s_network_plugin in ['calico']) | bool }}
  serviceMonitor:
    enabled: true
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
kubeStateMetrics:
  enabled: true
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

##
kube-state-metrics:
  priorityClassName: "system-cluster-critical"
  rbac:
    create: true
    pspEnabled: false
{% if monitoring_common_labels %}
  customLabels:
{% for label_key, label_value in monitoring_common_labels.items() %}
    {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}

##
nodeExporter:
  enabled: true
  ## Use the value configured in prometheus-node-exporter.podLabels
  jobLabel: jobLabel

## Configuration for prometheus-node-exporter subchart
##
prometheus-node-exporter:
  priorityClassName: "system-node-critical"
  namespaceOverride: ""
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    ##
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  prometheus:
    monitor:
      enabled: true
{% if monitoring_common_labels %}
      additionalLabels:
{% for label_key, label_value in monitoring_common_labels.items() %}
        {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          separator: ;
          regex: ^(.*)$
          targetLabel: nodename
          replacement: $1
          action: replace

##
prometheusOperator:
  enabled: true
  priorityClassName: "system-cluster-critical"
  admissionWebhooks:
    patch:
      priorityClassName: "system-cluster-critical"
  resources:
    limits:
      cpu: {{ monitoring_prometheus_operator_cpu_limit | to_json }}
      memory: {{ monitoring_prometheus_operator_memory_limit | to_json }}
    requests:
      cpu: {{ monitoring_prometheus_operator_cpu_request | to_json }}
      memory: {{ monitoring_prometheus_operator_memory_request | to_json }}
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
{% if monitoring_scheduling_key %}
  tolerations:
    - key: "{{ monitoring_scheduling_key }}"
      operator: Exists
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "{{ monitoring_scheduling_key }}"
                operator: Exists
{% endif %}

##
prometheus:
  enabled: true
  thanosService:
    enabled: {{ monitoring_use_thanos | bool }}
  thanosServiceMonitor:
    enabled: {{ monitoring_use_thanos | bool }}
  serviceMonitor:
    relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
  prometheusSpec:
    priorityClassName: "system-cluster-critical"
    secrets:
      - etcd-metrics-proxy
    serviceMonitorSelectorNilUsesHelmValues: {{  monitoring_common_labels | ternary(true, false) }}
{% if monitoring_common_labels %}
    serviceMonitorSelector:
      matchLabels:
{% for label_key, label_value in monitoring_common_labels.items() %}
        {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}
{% if monitoring_use_thanos | bool %}
    thanos:
      objectStorageConfig:
        optional: false
        name: thanos-objectstorage
        key: thanos.yaml
{% endif %}
    containers:
      - name: prometheus
        readinessProbe:
          failureThreshold: 1000
    resources:
      requests:
        cpu: "{{ monitoring_prometheus_cpu_request }}"
        memory: "{{ monitoring_prometheus_memory_request }}"
      limits:
        cpu: "{{ monitoring_prometheus_cpu_limit }}"
        memory: "{{ monitoring_prometheus_memory_limit }}"
{% if monitoring_scheduling_key %}
    tolerations:
    - key: "{{ monitoring_scheduling_key }}"
      operator: Exists
      effect: NoSchedule
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "{{ monitoring_scheduling_key }}"
                  operator: Exists
{% endif %}
