# The purpose of this role is to expose etcd's metrics in somewhat secure manner.
# etcd offers metrics
# - on 127.0.0.1:2381/metrics without authentication.
# - on 0.0.0.0:2379/metrics with authentication
# That's sensible because etcd contains sensitive information and we mustn't risk exposing
# it to the internet. So what's the deal? We cannot use the 2379/TCP endpoint, because
# in our setup etcd doesn't have any RBAC. If you're able to authenticate with your client
# cert, then you can execute any operation on etcd. We don't want to give such power to our monitoring.
# Instead we'll create a DaemonSet with socat which runs on the controller nodes in the host network
# so that it can access 127.0.0.1:2381. To the outside, it relays these metrics on 0.0.0.0:12381 with
# cert-based auth. See the difference? If you can authenticate against our DS, then you can only read
# metrics but not control etcd. That's an improvement, isn't it?
#
# A word on certificates. We cannot rely on cert-manager because its deployment is optional thusfar
# and if we would want to make it mandatory, then we'd have to tread carefully not to interfere with
# any user-controlled cert-manager instance.
# Even though we now have support for Vault, we chose not to handle this in Vault, for one simple
# reason: Golang does not support CRLs. Without setting up and making reachable an OCSP endpoint,
# using a CA actually degrades security over (even irregularly) rotated keypairs: A keypair will
# stay valid for its entire lifetime and there's no way to stop it from being valid. To invalidate
# a keypair, we'd have to rotate *and replace* the associated CA, and with that we're back to square
# one and can just go ahead and have a "manually" managed self-signed and explicitly trusted keypair.
#
# This role creates two self-signed certificates ("server" -> socat, "client" -> monitoring). The client cert acts as
# CA on the server-side and vice versa.
# Rotation: There's no need to expire a certificate because there's no common CA. Just delete either of the
# secrets and the LCM will create a fresh pair.
- name: Check if the etcd-metrics-proxy cert secret already exists (kube-system)
  kubernetes.core.k8s_info:
      api_version: v1
      kind: Secret
      namespace: kube-system
      name: etcd-metrics-proxy
  register: server_cert_state

- name: Check if the etcd-metrics-proxy cert secret already exists (monitoring)
  kubernetes.core.k8s_info:
      api_version: v1
      kind: Secret
      namespace: monitoring
      name: etcd-metrics-proxy
  register: client_cert_state

# We need a "server" certificate for the etcd-metrics-proxy endpoint
# and a "client" certificate for the prometheus scraper.
# TODO: Replace me with cert-manager / vault
- name: Create the certs if they don't exist yet
  when: not (server_cert_state.resources and client_cert_state.resources)
  block:
    - name: Get the controller nodes
      # Used to shorten the `label_selectors` expression
      vars:
        old: node-role.kubernetes.io/master
        new: node-role.kubernetes.io/control-plane
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        # K8s v1.20 introduces "node-role.kubernetes.io/control-plane" as a new label to denote controller denotes.
        # https://github.com/kubernetes/kubeadm/issues/2200
        # label_selectors are AND'ed.
        label_selectors:
          - "{{ k8s_version is version('1.20.0', '<', version_type='semver') | ternary(old, new) }}"
      register: controller_nodes
    - name: Create the x509 key of the server
      community.crypto.openssl_privatekey_pipe:
      no_log: true
      register: server_key
    - name: Generate the CSR of the server
      community.crypto.openssl_csr_pipe:
        privatekey_content: "{{ server_key.privatekey }}"
        common_name: 127.0.0.1
        subject:
          CN: 127.0.0.1
        # Some ansible foo to get the IPs of all controller nodes, prefixed by `IP:` as SANs
        # Maybe someone knows a shorter way? :)
        subject_alt_name: "{{ (controller_nodes.resources | map(attribute='status') | map(attribute='addresses') | flatten(levels=1) | selectattr('type', 'eq', 'InternalIP') | map(attribute='address')) | map('regex_replace', '^(.*)$', 'IP:\\1') | list }}"
      register: server_csr
    - name: Create the x509 certificate of the server
      community.crypto.x509_certificate_pipe:
        provider: selfsigned
        csr_content: "{{ server_csr.csr }}"
        privatekey_content: "{{ server_key.privatekey }}"
        entrust_not_after: "+10000d" # See the comment at the top about rotation
      register: server_cert

    - name: Create the x509 key of the client
      community.crypto.openssl_privatekey_pipe:
      no_log: true
      register: client_key
    - name: Generate the CSR of the client
      community.crypto.openssl_csr_pipe:
        privatekey_content: "{{ client_key.privatekey }}"
        common_name: 127.0.0.1
        subject:
          CN: 127.0.0.1
      register: client_csr
    - name: Create the x509 certificate of the client
      community.crypto.x509_certificate_pipe:
        provider: selfsigned
        csr_content: "{{ client_csr.csr }}"
        privatekey_content: "{{ client_key.privatekey }}"
        entrust_not_after: "+10000d" # See the comment at the top about rotation
      register: client_cert

    - name: Create the secret for the etcd-metrics-proxy endpoint
      kubernetes.core.k8s:
        apply: yes
        definition:
          apiVersion: v1
          kind: Secret
          type: Opaque
          data:
            client.crt: "{{ client_cert.certificate | b64encode }}"
            server.crt: "{{ server_cert.certificate | b64encode }}"
            server.key: "{{ server_key.privatekey | b64encode }}"
          metadata:
            name: etcd-metrics-proxy
            namespace: kube-system
        validate:
          fail_on_error: yes
          strict: yes

    - name: Create the secret for the prometheus scraper
      kubernetes.core.k8s:
        apply: yes
        definition:
          apiVersion: v1
          kind: Secret
          type: Opaque
          data:
            client.crt: "{{ client_cert.certificate | b64encode }}"
            client.key: "{{ client_key.privatekey | b64encode }}"
            server.crt: "{{ server_cert.certificate | b64encode }}"
          metadata:
            name: etcd-metrics-proxy
            namespace: monitoring
        validate:
          fail_on_error: yes
          strict: yes

    # Restarting pods after configuration changes is messy business.
    # Note: I'd love to use handlers here but as the operation is split in two [0]
    # and ansible does not support blocks nor import/include_role in handlers [1][2]
    # I have to put the tasks here. One could argue that the etcd-metrics-proxy DS is in any case
    # already be deployed when the handler is run. To be consistent I'm putting the same logic here, however.
    # [0] In a fresh setup, the certificate secrets are created *before* prometheus. If prometheus doesn't exist yet,
    #     we're good and must not attempt to restart the StatefulSet. If it does exist, then trigger the rollout.
    # [1] https://github.com/ansible/ansible/issues/14270
    # [2] https://github.com/ansible/ansible/issues/20493
    - name: Restart prometheus
      block:
        - name: Is prometheus already deployed?
          kubernetes.core.k8s_info:
            kind: StatefulSet
            namespace: monitoring
            name: prometheus-prometheus-stack-kube-prom-prometheus
          register: prometheus

        - name: Restart prometheus
          when: prometheus.resources
          command: "kubectl rollout restart statefulset -n monitoring prometheus-prometheus-stack-kube-prom-prometheus"

    - name: Restart etcd-metrics-proxy
      block:
        - name: Is the etcd-metrics-proxy already deployed?
          kubernetes.core.k8s_info:
            kind: DaemonSet
            namespace: kube-system
            name: etcd-metrics-proxy
          register: etcd_metrics_proxy

        - name: Restart etcd-metrics-proxy
          when: etcd_metrics_proxy.resources
          command: "kubectl rollout restart daemonset -n kube-system etcd-metrics-proxy"

- name: Deploy the proxy
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: "{{ lookup('template', 'proxy.yaml' ) }}"
    validate:
      fail_on_error: yes
      strict: yes
