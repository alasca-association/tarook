---
- name: configure jsonnet
  tags:
    - k8s-monitoring-build
  ansible.builtin.template:
    src: monitoring.jsonnet
    dest: "{{ monitoring_jsonnet_dir_monitoring }}/config.jsonnet"
    mode: 0664

- name: rebuild
  tags:
    - k8s-monitoring-build
  ansible.builtin.command:
  args:
    argv:
    - "make"
    - "-f"
    - "../Makefile"
    - "build"
    chdir: "{{ monitoring_jsonnet_dir_monitoring }}"
  changed_when: false

- name: check if configmap for customer dashboards exists
  kubernetes.core.k8s_info:
    api_version: v1
    kind: ConfigMap
    name: "{{ monitoring_grafana_customer_dashboards_configmap }}"
    namespace: monitoring
  register: customer_dashboard

- name: create an empty configmap for customer dashboards
  when: monitoring_grafana_customer_dashboards and not customer_dashboard.resources
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: {{ monitoring_grafana_customer_dashboards_configmap | to_json }}
        namespace: monitoring
    validate:
      fail_on_error: yes
      strict: yes
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"

- name: apply manifests
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: "{{ lookup('file', monitoring_jsonnet_dir_monitoring + '/' + item) }}"
    validate:
      fail_on_error: yes
      strict: yes
  loop: "{{ lookup('file', monitoring_jsonnet_dir_monitoring + '/manifests.files.json', errors='strict' if (k8s_monitoring_enabled and monitoring_use_jsonnet_setup) else 'warn') | default(' [] ', true) | from_json }}"  # noqa 204
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"

- name: create haproxy service monitor (gateway)
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: "{{ lookup('template', 'haproxy-service-monitor.yaml') }}"
    validate:
      fail_on_error: yes
      strict: yes
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"
  tags:
    - haproxy-service-monitor

- name: create keepalived service monitor (gateway)
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: "{{ lookup('template', 'keepalived-service-monitor.yaml') }}"
    validate:
      fail_on_error: yes
      strict: yes
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"
  tags:
    - keepalived-service-monitor

- name: create node-exporter service monitor (gateway)
  kubernetes.core.k8s:
    state: present
    apply: yes
    definition: "{{ lookup('template', 'node-exporter-service-monitor.yaml') }}"
    validate:
      fail_on_error: yes
      strict: yes
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"
  tags:
    - node-exporter-service-monitor

- name: create LBaaS component service monitors (gateway)
  kubernetes.core.k8s:
    definition: "{{ lookup('template', item) }}"
    apply: yes
    validate:
      fail_on_error: yes
      strict: yes
  with_items:
  - lbaas-service-monitor.yaml
  when: ch_k8s_lbaas_agent_port
  # Retry this task on failures
  register: k8s_apply
  until: k8s_apply is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"
  tags:
  - ch-k8s-lbaas
  - ch-k8s-lbaas-agent
  - ch-k8s-lbaas-controller
...
