# Variables marked with "•ᴗ•" are mandatory!
# This means you have to set them, there is no default value!
# If a variable is commented out, it is optional and the defined
# value is the default.

# NOTE:For some sections the inventory updater will apply prefixes to
# the variables before passing them over to the ansible inventories.

# ANCHOR: load-balancing_config
# --- LOAD-BALANCING ---
# ansible prefix: /
[load-balancing]
# lb_ports is a list of ports that are exposed by HAProxy on the gateway nodes and forwarded
# to NodePorts in the k8s cluster. This poor man's load-balancing / exposing of services
# has been superseded by ch-k8s-lbaas. For legacy reasons and because it's useful under
# certain circumstances it is kept inside the repository.
# The NodePorts are either literally exposed by HAProxy or can be mapped to other ports.
# The `layer` attribute can either be `tcp` (L4) or `http` (L7). For `http`, `option forwardfor`
# is added implicitly to the backend servers in the haproxy configuration.
# If `use_proxy_protocol` is set to `true`, HAProxy will use the proxy protocol to convey information
# about the connection initiator to the backend. NOTE: the backend has to accept the proxy
# protocol, otherwise your traffic will be discarded.
# Short form:
#lb_ports = [30060]
# Explicit form:
#lb_ports = [{external=80,nodeport=30080, layer=tcp, use_proxy_protocol=true}]

# A list of priorities to assign to the gateway/frontend nodes. The priorities
# will be assigned based on the sorted list of matching nodes.
#
# If more nodes exist than there are entries in this list, the rollout will
# fail.
#
# Please note the keepalived.conf manpage for choosing priority values.
#vrrp_priorities = [150, 100, 50]

# Enable/Disable OpenStack-based load-balancing.
# openstack_lbaas = false

# Port for HAProxy statistics
#haproxy_stats_port = 48981

# ANCHOR_END: load-balancing_config

# ANCHOR: kubernetes_basic_cluster_configuration
# --- KUBERNETES: BASIC CLUSTER CONFIGURATION ---
# ansible prefix: "k8s_"
[kubernetes]
# Kubernetes version. Currently, we support from 1.19.* to 1.23.*.
version = "1.24.10" # •ᴗ•

# Uncomment if this cluster contains a worker with GPU access so that the driver
# and surrounding framework is deployed.
# You must set the container_runtime to "docker" for GPU clusters.
is_gpu_cluster = false # •ᴗ•

# Set this variable to virtualize Nvidia GPUs on worker nodes.
# It will install a VGPU manager on the worker node and split the GPU according to chosen vgpu type.
# Note: This will not install Nvidia drivers to utilize vGPU guest VMs!!
# If set to true, please set further variables in the [miscellaneous] section.
#virtualize_gpu = false

[kubernetes.apiserver]
frontend_port = 8888 # •ᴗ•

[kubernetes.controller_manager]
#large_cluster_size_threshold = 50

# ANCHOR_END: kubernetes_basic_cluster_configuration

# ANCHOR: kubernetes_network_configuration
# --- KUBERNETES: NETWORK CONFIGURATION ---
# ansible prefix: "k8s_network"
[kubernetes.network]
# This is the subnet used by Kubernetes for Pods. Subnets will be delegated
# automatically to each node.
pod_subnet = "10.244.0.0/16" # •ᴗ•

# This is the subnet used by Kubernetes for Services.
service_subnet = "10.96.0.0/12" # •ᴗ•

# calico:
# High-performance, pure IP networking, policy engine. Calico provides
# layer 3 networking capabilities and associates a virtual router with each node.
# Allows the establishment of zone boundaries through BGP
plugin = "calico" # •ᴗ•

# Pick a Calico version:
# Be aware that not all combinations of Kubernetes and Calico versions are recommended:
# https://projectcalico.docs.tigera.io/getting-started/kubernetes/requirements
# We provide Calico in version: 3.17.1, 3.19.0, 3.21.6.
# If not specified here, a predefined Calico version will be matched against
# the above specified Kubernetes version.
# calico_custom_version = "3.21.6"

# Define if the IP-in-IP encapsulation of calico should be activated
# https://projectcalico.docs.tigera.io/networking/vxlan-ipip
# calico_ipipmode = "Never"

# Make the auto detection method variable as one downside of
# using can-reach mechanism is that it produces additional logs about
# other interfaces i.e. tap interfaces. Also a simpler way will be to
# use an interface to detect ip settings i.e. interface=bond0
# calico_ip_autodetection_method = "can-reach=www.cloudandheat.com"
# calico_ipv6_autodetection_method = "can-reach=www.cloudandheat.com"
# ANCHOR_END: kubernetes_network_configuration

# ANCHOR: kubernetes_kubelet_configuration
# --- KUBERNETES: KUBELET CONFIGURATION (WORKERS) ---
# ansible prefix: "k8s_kubelet"
[kubernetes.kubelet]
# This section enables you to customize kubelet on the k8s workers (sic!)
# Changes will be rolled out only during k8s upgrades or if you explicitly
# allow disruptions.

# Maximum number of Pods per worker
# Increasing this value may also decrease performance,
# as more Pods can be packed into a single node.
#pod_limit = 110

# ANCHOR_END: kubernetes_kubelet_configuration

# ANCHOR: kubernetes_continuous_join_key_configuration
# --- KUBERNETES: CONTINUOUS JOIN KEY ---
# ansible prefix: "k8s_continuous_join_key"
[kubernetes.continuous_join_key]
# This section controls a systemd timer which periodically publishes a fresh
# Kubernetes join key into a HashiCorp Vault server. At the time of writing,
# the primary use case is integration with the scripts provided to every node
# deployed via the Yaook Metal Controller.
enabled = false # •ᴗ•

# Absolute path to an executable which prints a fresh, revocable vault token
# to stdout. This token must be privileged enough to write the given vault path
# below.
#vault_token_script =

# Path to a Vault key/value object where the join key data should be written
# to.
#vault_path =

# Path to a file which should be loaded as additional environment to the token
# renewal script.
#env_path = "/dev/null"

# ANCHOR_END: kubernetes_continuous_join_key_configuration

# ANCHOR: ipsec_configuration
# --- IPSEC ---
# ansible prefix: "ipsec_"
[ipsec]
# enabled = false

# Flag to enable the test suite.
# Must make sure a remote endpoint, with ipsec enabled, is running and open for connections.
# test_enabled = false

# Must be a list of parent SA proposals to offer to the client.
# Must be explicitly set if ipsec_enabled is set to true.
#proposals =

# Must be a list of ESP proposals to offer to the client.
#esp_proposals = "{{ ipsec_proposals }}"

# List of CIDRs to route to the peer. If not set, only dynamic IP
# assignments will be routed.
#peer_networks = []

# List of CIDRs to offer to the peer.
#local_networks = ["{{ subnet_cidr }}"]

# Pool to source virtual IP addresses from. Those are the IP addresses assigned
# to clients which do not have remote networks. (e.g.: "10.3.0.0/24")
#virtual_subnet_pool = null

# List of addresses to accept as remote. When initiating, the first single IP
# address is used.
#remote_addrs = false

# Private address of remote endpoint.
# only used when test_enabled is True
#remote_private_addrs = ""

# The PSK for EAP. Must be set.
#eap_psk =

# ANCHOR_END: ipsec_configuration

# ANCHOR: testing_test_nodes_configuration
# --- TESTING: TEST NODES ---
[testing.test-nodes]
# The following fields are commented out because they make assumptions on the existence
# and naming scheme of nodes. Use them for inspiration :)
#"managed-k8s-worker-1" = "worker0"
#"managed-k8s-worker-3" = "worker1"
#"managed-k8s-worker-5" = "worker2"

# ANCHOR_END: testing_test_nodes_configuration

# ANCHOR: miscellaneous_configuration
# --- MISCELLANEOUS ---
# ansible prefix: /
[miscellaneous]

# Configure which container runtime to use.
# Supported values are: "docker" and "containerd".
# For now migrating the container runtime of existing clusters is not
# supported.
container_runtime = "containerd"  # "•ᴗ•"

# Configuration details if the cluster will be placed behind a HTTP proxy.
# If unconfigured images will be used to setup the cluster, the updates of
# package sources, the download of docker images and the initial cluster setup will fail.
# NOTE: These chances are currently only tested for Debian-based operating systems and not for RHEL-based!
#cluster_behind_proxy = false
# Set the approriate HTTP proxy settings for your cluster here. E.g. the address of the proxy or
# internal docker repositories can be added to the no_proxy config entry
# Important note: Settings for the yaook-k8s cluster itself (like the service subnet or the pod subnet)
# will be set automagically and do not have to set manually here.
#http_proxy = "http://proxy.example.com:8889"
#https_proxy = "http://proxy.example.com:8889"
#no_proxy = "localhost,127.0.0.0/8"

# Name of the internal OpenStack network. This field becomes important if a VM is
# attached to two networks but the controller-manager should only pick up one. If
# you don't understand the purpose of this field, there's a very high chance you
# won't need to touch it/uncomment it.
# Note: This network name isn't fetched automagically (by terraform) on purpose
# because there might be situations where the CCM should not pick the managed network.
#openstack_network_name = "managed-k8s-network"

# Value for the kernel parameter `vm.max_map_count` on k8s worker nodes. Modifications
# might be required depending on the software running on the nodes (e.g., ElasticSearch).
# If you leave the value commented out you're fine and the system's default will be kept.
#vm_max_map_count = 262144

# Custom Docker Configuration
# A list of registry mirrors can be configured as a pull through cache to reduce
# external network traffic and the amount of docker pulls from dockerhub.
#registry_mirrors: [ "https://0.docker-mirror.example.org", "https://1.docker-mirror.example.org" ]

# A list of insecure registries that can be accessed without TLS verification.
#insecure_registries: [ "0.docker-registry.example.org", "1.docker-registry.example.org" ]

# ANCHOR_END: miscellaneous_configuration
