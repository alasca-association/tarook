# Variables marked with "•ᴗ•" are mandatory!
# This means you have to set them, there is no default value!
# If a variable is commented out, it is optional and the defined
# value is the default.

# NOTE:For some sections the inventory updater will apply prefixes to
# the variables before passing them over to the ansible inventories.

# ANCHOR: terraform_config
# --- TERRAFORM ---
# ansible prefix: /
[terraform]
#subnet_cidr = "172.30.154.0/24"
#masters = 3
#workers = 3
#worker_flavors = ["L", "M", "L"]

# if set to true one must set proper values for the "azs" array according to the cloud in use
#enable_az_management = false

# Enable DualStack support
# WARNING: DualStack support is not stable yet, see https://gitlab.com/yaook/k8s/-/issues/502
dualstack_support = false # "•ᴗ•"
# If you enabled DualStack-support you may want to adjust the IPv6 subnet
#subnet_v6_cidr = "fd00::/120"

# If true, create block volume for each instance and boot from there.
# Equivalent to `openstack server create --boot-from-volume […].
#create_root_disk_on_volume = false

# Volume type that is used if `create_root_disk_on_volume` is true.
#root_disk_volume_type = "three_times_replicated"

# ANCHOR_END: terraform_config

# ANCHOR: vault_configuration
# --- VAULT BACKEND ---
# ansible prefix: vault_
[vault]
# Name of the cluster inside Vault. The secrets engines are searched for
# relative to $path_prefix/$cluster_name/.
# This name must be unique within a single vault instance and cannot be
# reasonably changed after a cluster has been spawned.
cluster_name = "devcluster" # "•ᴗ•"

# ANCHOR_END: vault_configuration

# ANCHOR: load-balancing_config
# --- LOAD-BALANCING ---
# ansible prefix: /
[load-balancing]
# lb_ports is a list of ports that are exposed by HAProxy on the gateway nodes and forwarded
# to NodePorts in the k8s cluster. This poor man's load-balancing / exposing of services
# has been superseded by ch-k8s-lbaas. For legacy reasons and because it's useful under
# certain circumstances it is kept inside the repository.
# The NodePorts are either literally exposed by HAProxy or can be mapped to other ports.
# The `layer` attribute can either be `tcp` (L4) or `http` (L7). For `http`, `option forwardfor`
# is added implicitly to the backend servers in the haproxy configuration.
# If `use_proxy_protocol` is set to `true`, HAProxy will use the proxy protocol to convey information
# about the connection initiator to the backend. NOTE: the backend has to accept the proxy
# protocol, otherwise your traffic will be discarded.
# Short form:
#lb_ports = [30060]
# Explicit form:
#lb_ports = [{external=80,nodeport=30080, layer=tcp, use_proxy_protocol=true}]

# A list of priorities to assign to the gateway/frontend nodes. The priorities
# will be assigned based on the sorted list of matching nodes.
#
# If more nodes exist than there are entries in this list, the rollout will
# fail.
#
# Please note the keepalived.conf manpage for choosing priority values.
#vrrp_priorities = [150, 100, 50]

# Enable/Disable OpenStack-based load-balancing.
# openstack_lbaas = false

# Port for HAProxy statistics
#haproxy_stats_port = 48981

# ANCHOR_END: load-balancing_config

# ANCHOR: ch-k8s-lbaas_config
# --- C&H KUBERNETES LBaaS ---
# ansible prefix: "ch_k8s_lbaas_"
[ch-k8s-lbaas]
# To enable our LBaaS service, un-comment the following options and fill in a
# unique, random, base64-encoded secret in place of `...`.
# To generate such a secret, you can use the following command:
# $ dd if=/dev/urandom bs=16 count=1 status=none | base64

enabled       = true    # •ᴗ•
shared_secret = "..."   # REPLACE ME
version       = "0.6.0"
agent_port    = 15203

# Configure which IP address ("port") manager to use. Two options are available:
# - openstack: Uses OpenStack and the yaook/k8s gateway nodes to provision
#   LBaaS IP addresses ports.
# - static: Uses a fixed set of IP addresses to use for load balancing. When the
#   static port manager is used, the `agent_urls` and `static_ipv4_addresses`
#   options must also be configured.
#port_manager  = "openstack"

# List of IPv4 addresses which are usable for the static port manager. It is
# your responsibility to ensure that the node(s) which run the agent(s) receive
# traffic for these IPv4 addresses.
#static_ipv4_addresses = []

# Customize URLs for the agents. This will typically be a list of HTTP URLs
# like http://agent_ip:15203. This option is only used if the port manager is
# set to `static`, and must be set if the port manager is `static`.
#agent_urls = []

# Configure memory and CPU resources for the lbaas controller
#controller_cpu_request = "100m"
#controller_memory_request = "256Mi"
#controller_cpu_limit = "200m"
#controller_memory_limit = "256Mi"

# ANCHOR_END: ch-k8s-lbaas_config

# ANCHOR: kubernetes_basic_cluster_configuration
# --- KUBERNETES: BASIC CLUSTER CONFIGURATION ---
# ansible prefix: "k8s_"
[kubernetes]
# Kubernetes version. Currently, we support from 1.24.* to 1.26.*.
version = "1.26.8" # •ᴗ•

# Set this variable if this cluster contains worker with GPU access
# and you want to make use of these inside of the cluster,
# so that the driver and surrounding framework is deployed.
is_gpu_cluster = false # •ᴗ•

# Set this variable to virtualize Nvidia GPUs on worker nodes
# for usage outside of the Kubernetes cluster / above the Kubernetes layer.
# It will install a VGPU manager on the worker node and
# split the GPU according to chosen vgpu type.
# Note: This will not install Nvidia drivers to utilize vGPU guest VMs!!
# If set to true, please set further variables in the [miscellaneous] section.
# Note: This is mutually exclusive with "is_gpu_cluster"
virtualize_gpu = false

[kubernetes.apiserver]
frontend_port = 8888 # •ᴗ•

[kubernetes.controller_manager]
#large_cluster_size_threshold = 50

# ANCHOR_END: kubernetes_basic_cluster_configuration

# ANCHOR: storage_base_configuration
# --- KUBERNETES: STORAGE CONFIGURATION ---
# ansible prefix: "k8s_storage"
[kubernetes.storage]
# Many clusters will want to use rook, so you should enable
# or disable it here if you want. It requires extra options
# which need to be chosen with care.
rook_enabled = false # •ᴗ•

# Setting this to true will cause the storage plugins
#to run on all nodes (ignoring all taints). This is often desirable.
nodeplugin_toleration = false # •ᴗ•

# This flag enables the topology feature gate of the cinder controller plugin.
# Its purpose is to allocate volumes from cinder which are in the same AZ as
# the worker node to which the volume should be attached.
# Important: Cinder must support AZs and the AZs must match the AZs used by nova!
#cinder_enable_topology=true

# ANCHOR_END: storage_base_configuration

# ANCHOR: storage_local_static_configuration
# --- KUBERNETES: STATIC LOCAL STORAGE CONFIGURATION ---
# ansible prefix: "k8s_local_storage"
[kubernetes.local_storage.static]
# Enable static provisioning of local storage. This provisions a single local
# storage volume per worker node.
#
# It is recommended to use the dynamic local storage instead.
enabled = false # •ᴗ•

# Name of the storage class to create.
#
# NOTE: the static and dynamic provisioner must have distinct storage class
# names if both are enabled!
#storageclass_name = "local-storage"

# Namespace to deploy the components in
#namespace = "kube-system"

# Directory where the volume will be placed on the worker node
#data_directory = "/mnt/data"

# Synchronization directory where the provisioner will pick up the volume from
#discovery_directory = "/mnt/mk8s-disks"

# Version of the provisioner to use
#version = "v2.3.4"

# Toleration for the plugin. Defaults to `kubernetes.storage.nodeplugin_toleration`
#nodeplugin_toleration = ...

# ANCHOR_END: storage_local_static_configuration

# ANCHOR: storage_local_dynamic_configuration
# --- KUBERNETES: DYNAMIC LOCAL STORAGE CONFIGURATION ---
# ansible prefix: "k8s_local_storage"
[kubernetes.local_storage.dynamic]
# Enable dynamic local storage provisioning. This provides a storage class which
# can be used with PVCs to allocate local storage on a node.
enabled = false # •ᴗ•

# Name of the storage class to create.
#
# NOTE: the static and dynamic provisioner must have distinct storage class
# names if both are enabled!
#storageclass_name = "local-storage"

# Namespace to deploy the components in
#namespace = "kube-system"

# Directory where the volumes will be placed on the worker node
#data_directory = "/mnt/dynamic-data"

# Version of the local path controller to deploy
#version = "v0.0.20"

# Toleration for the plugin. Defaults to `kubernetes.storage.nodeplugin_toleration`
#nodeplugin_toleration = ...

# ANCHOR_END: storage_local_dynamic_configuration

# ANCHOR: kubernetes_monitoring_configuration
# --- KUBERNETES: MONITORING CONFIGURATION ---
# ansible prefix: "k8s_monitoring"
[kubernetes.monitoring]
# Enable Prometheus-based monitoring.
# For prometheus-specific configurations take a look at the
# k8s-service-layer.prometheus section.
enabled = false # •ᴗ•

# ANCHOR_END: kubernetes_monitoring_configuration

# ANCHOR: kubernetes_global_monitoring_configuration
# --- KUBERNETES: GLOBAL MONITORING CONFIGURATION ---
# ansible prefix: "k8s_global_monitoring"
[kubernetes.global_monitoring]
# This section contains global monitoring related
# information which needs to be known to stage3
# and higher layers.

# Enable/Disable global monitoring
enabled       = false                      # •ᴗ•
#nodeport      = 31911
#nodeport_name = "ch-k8s-global-monitoring"

# ANCHOR_END: kubernetes_global_monitoring_configuration

# ANCHOR: kubernetes_network_configuration
# --- KUBERNETES: NETWORK CONFIGURATION ---
# ansible prefix: "k8s_network"
[kubernetes.network]
# This is the subnet used by Kubernetes for Pods. Subnets will be delegated
# automatically to each node.
pod_subnet = "10.244.0.0/16" # •ᴗ•

# This is the subnet used by Kubernetes for Services.
service_subnet = "10.96.0.0/12" # •ᴗ•

# calico:
# High-performance, pure IP networking, policy engine. Calico provides
# layer 3 networking capabilities and associates a virtual router with each node.
# Allows the establishment of zone boundaries through BGP
plugin = "calico" # •ᴗ•
# ANCHOR_END: kubernetes_network_configuration

# ANCHOR: calico_configuration
[kubernetes.network.calico]
#mtu = 1450 # for OpenStack at most 1450

# Only takes effect for operator-based installations
# https://docs.tigera.io/calico/3.25/reference/installation/api#operator.tigera.io/v1.EncapsulationType
#encapsulation = "None"

# Only takes effect for manifest-based installations
# Define if the IP-in-IP encapsulation of calico should be activated
# https://docs.tigera.io/calico/3.24/reference/resources/ippool#spec
#ipipmode = "Never"

# Make the auto detection method variable as one downside of
# using can-reach mechanism is that it produces additional logs about
# other interfaces i.e. tap interfaces. Also a simpler way will be to
# use an interface to detect ip settings i.e. interface=bond0
# calico_ip_autodetection_method = "can-reach=www.cloudandheat.com"
# calico_ipv6_autodetection_method = "can-reach=www.cloudandheat.com"

# For the operator-based installation,
# it is possible to link to self-maintained values file for the helm chart
#values_file_path = "path-to-a-custom/values.yaml"

# We're mapping a fitting calico version to the configured Kubernetes version.
# You can however pick a custom Calico version.
# Be aware that not all combinations of Kubernetes and Calico versions are recommended:
# https://projectcalico.docs.tigera.io/getting-started/kubernetes/requirements
# For the operator-based installation, any version should work as long as
# you stick to the calico-Kubernetes compatibility matrix.
# # For the deprecated manifest-based installation we provide Calico in version: 3.17.1, 3.19.0, 3.21.6, 3.24.5
#
# If not specified here, a predefined Calico version will be matched against
# the above specified Kubernetes version.
#custom_version = "3.25.1"

# Calico can be either installed via a manually implemented manifest-based approach
# (default for K8s <= v1.24) or via the tigera operator (enforced for K8s >= v1.25).
# Migration from the previous to the latter happens automatically, if this variable
# here is enabled. Note that this is considered disruptive.
# For more information refer to: https://docs.tigera.io/calico/3.25/operations/operator-migration
# and our documentation: https://yaook.gitlab.io/k8s/
#use_tigera_operator = true

# ANCHOR_END: calico_configuration

# ANCHOR: kubernetes_kubelet_configuration
# --- KUBERNETES: KUBELET CONFIGURATION (WORKERS) ---
# ansible prefix: "k8s_kubelet"
[kubernetes.kubelet]
# This section enables you to customize kubelet on the k8s workers (sic!)
# Changes will be rolled out only during k8s upgrades or if you explicitly
# allow disruptions.

# Maximum number of Pods per worker
# Increasing this value may also decrease performance,
# as more Pods can be packed into a single node.
#pod_limit = 110

# Config for soft and hard eviction values.
# Note: To change these values you have to release the Kraken
#evictionsoft_memory_available = "30s"
#evictionhard_nodefs_available = "10%"
#evictionhard_nodefs_inodesfree = "5%"

# ANCHOR_END: kubernetes_kubelet_configuration

# --- KUBERNETES SERVICE LAYER ---
# ANCHOR: ksl_rook_configuration
# --- KUBERNETES SERVICE LAYER : ROOK (STORAGE) ---
# ansible prefix: "rook"
[k8s-service-layer.rook]
# If kubernetes.storage.rook_enabled is enabled, rook will be installed.
# In this section you can customize and configure rook.

namespace    = "rook-ceph" # •ᴗ•
cluster_name = "rook-ceph" # •ᴗ•
use_helm = true

# Configure a custom Ceph version.
# If not defined, the one mapped to the rook version
# will be used. Be aware that you can't choose an
# arbitrary Ceph version, but should stick to the
# rook-ceph-compatibility-matrix.
#custom_ceph_version = ""

# Currently we support the following rook versions:
# v1.2.3, v1.3.11, v1.4.9, v1.5.12, v1.6.7, v1.7.11, v1.8.10
#version = "v1.7.11"

#nodeplugin_toleration = true

# Storage class name. SHOULD be compliant with one storage class you
# have configured in the kubernetes.local_storage section (or you should
# know what your are doing).
#mon_volume_storage_class = "local-storage"

# Enables rook to use the host network.
#use_host_networking = false

# If set to true Rook won’t perform any upgrade checks on Ceph daemons
# during an upgrade. Use this at YOUR OWN RISK, only if you know what
# you’re doing.
# https://rook.github.io/docs/rook/v1.3/ceph-cluster-crd.html#cluster-settings
#skip_upgrade_checks = false

# If true, the rook operator will create and manage PodDisruptionBudgets
# for OSD, Mon, RGW, and MDS daemons.
#rook_manage_pod_budgets = true

# Scheduling keys control where services may run. A scheduling key corresponds
# to both a node label and to a taint. In order for a service to run on a node,
# it needs to have that label key.
# If no scheduling key is defined for a service, it will run on any untainted
# node.
#scheduling_key = null
# If you're using a general scheduling key prefix,
# you can reference it here directly:
#scheduling_key = "{{ scheduling_key_prefix }}/storage"

# Set to false to disable CSI plugins, if they are not needed in the rook cluster.
# (For example if the ceph cluster is used for an OpenStack cluster)
#csi_plugins=true

# Additionally it is possible to schedule mons and mgrs pods specifically.
# NOTE: Rook does not merge scheduling rules set in 'all' and the ones in 'mon' and 'mgr',
# but will use the most specific one for scheduling.
#mon_scheduling_key = "{{ scheduling_key_prefix }}/rook-mon"
#mgr_scheduling_key = "{{ scheduling_key_prefix }}/rook-mgr"

# Number of mons to run.
# Default is 3 and is the minimum to ensure high-availability!
# The number of mons has to be uneven.
#nmons = 3

# Number of mgrs to run. Default is 1 and can be extended to 2
# and achieve high-availability.
# The count of mgrs is adjustable since rook v1.6 and does not work with older versions.
#nmgrs = 1

# Number of OSDs to run. This should be equal to the number of storage meta
# workers you use.
#nosds = 3

# The size of the storage backing each OSD.
#osd_volume_size = "90Gi"

# Enable the rook toolbox, which is a pod with ceph tools installed to
# introspect the cluster state.
#toolbox = true

# Enable the CephFS shared filesystem.
#ceph_fs = false
#ceph_fs_name = "ceph-fs"
#ceph_fs_replicated = 1
#ceph_fs_preserve_pools_on_delete = false

# Enable the encryption of OSDs
#encrypt_osds = false

# ROOK POD RESOURCE LIMITS
# The default values are the *absolute minimum* values required by rook. Going
# below these numbers will make rook refuse to even create the pods. See also:
# https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#cluster-wide-resources-configuration-settings

# Memory limit for mon Pods
#mon_memory_limit = "1Gi"
#mon_memory_request = "{{ rook_mon_memory_limit }}"
#mon_cpu_limit = "500m"
#mon_cpu_request = "100m"

# Resource limits for OSD pods
# Note that these are chosen so that the OSD pods end up in the
# Guaranteed QoS class.
#osd_memory_limit = "2Gi"
#osd_memory_request = "{{ rook_osd_memory_limit }}"
#osd_cpu_limit = "500m"
#osd_cpu_request = "{{ rook_osd_cpu_limit }}"

# Memory limit for mgr Pods
#mgr_memory_limit = "512Mi"
#mgr_memory_request = "{{ rook_mgr_memory_limit }}"
#mgr_cpu_limit = "500m"
#mgr_cpu_request = "100m"

# Memory limit for MDS / CephFS Pods
#mds_memory_limit = "4Gi"
#mds_memory_request = "{{ rook_mds_memory_limit }}"
#mds_cpu_limit = "1"
#mds_cpu_request = "{{ rook_mds_cpu_limit }}"

# Rook-ceph operator limits
#operator_memory_limit: "512Mi"
#operator_memory_request: "{{ rook_operator_memory_limit }}"
#operator_cpu_limit = "1"
#operator_cpu_request = "{{ rook_operator_cpu_limit }}"

#[[k8s-service-layer.rook.pools]]
#name = "data"
#create_storage_class = "block"
#replicated = 1

# Custom storage configuration is documented at
# docs/src/managed-services/rook/custom-storage.md
#on_openstack = true
#use_all_available_devices = true
#use_all_available_nodes = true

# ANCHOR_END: ksl_rook_configuration

# ANCHOR: ksl_prometheus_configuration
# --- KUBERNETES SERVICE LAYER : MONITORING(PROMETHEUS) ---
# ansible prefix: "monitoring_"
[k8s-service-layer.prometheus]
# If kubernetes.monitoring.enabled is true, choose whether to install or uninstall
# Prometheus. IF SET TO FALSE, PROMETHEUS WILL BE DELETED WITHOUT CHECKING FOR
# DISRUPTION (sic!).
#install = true

#namespace = "monitoring"

# helm chart version of the prometheus stack
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
# If you set this empty (not unset), the latest version is used
# Note that upgrades require additional steps and maybe even LCM changes are needed:
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#upgrading-chart
#prometheus_stack_version = "48.1.1"

# Enable grafana
#use_grafana = true

# If this variable is defined, Grafana will store its data in a PersistentVolume
# in the defined StorageClass. Otherwise, persistence is disabled for Grafana.
# The value has to be a valid StorageClass available in your cluster.
#grafana_persistent_storage_class=""

# Enable use of Thanos
# Important: Currently Thanos will not work with OpenStack application credentials [0]. If you use OpenStack
# application credentials then you have to set `use_thanos = false`. f you don't listen to me, then the
# LCM will kindly throw an error message into your face in stage 4 and abort :).
#
# [0] https://gitlab.com/yaook/k8s/-/issues/436#note_873556688
#use_thanos = false

# Use the bitnami helm chart to deploy thanos. If use_helm_thanos is false the deployment will be done with the help
# of the deprecated jsonnet code.
# use_helm_thanos = false

# Let terraform create an object storage container / bucket for you if `true`.
# If set to `false` one must provide a value for `thanos_objectstorage_config_file` to configure an external backend.
# NOTE: If `thanos_objectstorage_config_file` is set, then it (`thanos_objectstorage_config_file`) will take precedence, i.e.,
#       thanos will use the external config although terraform creates the bucket. This has historic reasons [0].
# [0] https://gitlab.com/yaook/k8s/-/merge_requests/635
#manage_thanos_bucket = true

# Thanos uses local storage to keep a copy of the metadata from the object store
# for faster access. The size and storage class for that volume can be
# configured:
#thanos_metadata_volume_size="10Gi"
#thanos_metadata_volume_storage_class="rook-ceph-data"

# By default, the monitoring will capture all namespaces. If this is not
# desired, the following switch can be turned off. In that case, only the
# kube-system, monitoring and rook namespaces are scraped by Prometheus.
#prometheus_monitor_all_namespaces=true

# How many replicas of the alertmanager should be deployed inside the cluster
#alertmanager_replicas=1

# Scheduling keys control where services may run. A scheduling key corresponds
# to both a node label and to a taint. In order for a service to run on a node,
# it needs to have that label key.
# If no scheduling key is defined for service, it will run on any untainted
# node.
#scheduling_key = "node-restriction.kubernetes.io/cah-managed-k8s-monitoring"
# If you're using a general scheduling key prefix
# you can reference it here directly
#scheduling_key = "{{ scheduling_key_prefix }}/monitoring"

# Monitoring pod resource limits
# PROMETHEUS POD RESOURCE LIMITS
# The following limits are applied to the respective pods.
# Note that the Prometheus limits are chosen fairly conservatively and may need
# tuning for larger and smaller clusters.
# By default, we prefer to set limits in such a way that the Pods end up in the
# Guaranteed QoS class (i.e. both CPU and Memory limits and requests set to the
# same value).

#alertmanager_memory_limit = "256Mi"
#alertmanager_memory_request = "{{ monitoring_alertmanager_memory_limit }}"
#alertmanager_cpu_limit = "100m"
#alertmanager_cpu_request = "{{ monitoring_alertmanager_cpu_limit }}"

#prometheus_memory_limit = "3Gi"
#prometheus_memory_request = "{{ monitoring_prometheus_memory_limit }}"
#prometheus_cpu_limit = "1"
#prometheus_cpu_request = "{{ monitoring_prometheus_cpu_limit }}"

#grafana_memory_limit = "512Mi"
#grafana_memory_request = "256Mi"
#grafana_cpu_limit = "500m"
#grafana_cpu_request = "100m"

#kube_state_metrics_memory_limit = "128Mi"
#kube_state_metrics_memory_request = "50Mi"
#kube_state_metrics_cpu_limit = "50m"
#kube_state_metrics_cpu_request = "20m"

#thanos_sidecar_memory_limit = "256Mi"
#thanos_sidecar_memory_request = "{{ monitoring_thanos_sidecar_memory_limit }}"
#thanos_sidecar_cpu_limit = "500m"
#thanos_sidecar_cpu_request = "{{ monitoring_thanos_sidecar_cpu_limit }}"

#thanos_query_memory_limit = "786Mi"
#thanos_query_memory_request = "128Mi"
#thanos_query_cpu_limit = "1"
#thanos_query_cpu_request = "100m"

#thanos_store_memory_limit = "2Gi"
#thanos_store_memory_request = "256Mi"
#thanos_store_cpu_limit = "500m"
#thanos_store_cpu_request = "100m"

# WARNING: If you have set terraform.cluster_name, you must set this
# variable to "${terraform.cluster_name}-monitoring-thanos-data".
# The default terraform.cluster_name is "managed-k8s" which is why the
# default object store container name is set to the following.
#thanos_objectstorage_container_name = "managed-k8s-monitoring-thanos-data"

# The following two variables are needed if you want thanos to use an object storage
# backend which is not managed by the LCM.
# Configuration file name which contains the credentials to access the (external) object storage
# as backend for thanos. You can find valid configuration formats here [0]
# NOTE: The configuration file is read relative to `thanos_objectstorage_config_path`.
# NOTE: This variable takes precedence over `manage_thanos_bucket`. If it's set, then thanos will always use the credentials from this file.
#       See the discussions in [1] for more details.
# [0] https://thanos.io/tip/thanos/storage.md/#supported-clients
# [1] https://gitlab.com/yaook/k8s/-/merge_requests/635
#thanos_objectstorage_config_file=""

# Path in which the LCM will look for your config file. Defaults to "./config/"
# NOTE: you probably don't have to override this variable.
#thanos_objectstorage_config_path = "{{ ksl_vars_directory }}/../../config/"

# Scrape external targets via blackbox exporter
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-blackbox-exporter
#internet_probe = false

# Provide a list of DNS endpoints for additional thanos store endpoints.
# The endpoint will be extended to `dnssrv+_grpc._tcp.{{ endpoint }}.monitoring.svc.cluster.local`.
#thanos_query_additional_store_endpoints = []

# Deploy a specific blackbox exporter version
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-blackbox-exporter
#blackbox_version = "7.0.0"

# By default, prometheus and alertmanager only consider global rules from the monitoring
# namespace while other rules can only alert on their own namespace. If this variable is
# set, cluster wide rules are considered from all namespaces.
#allow_external_rules = false

#[[k8s-service-layer.prometheus.internet_probe_targets]]
# name="example"                    # Human readable URL that will appear in Prometheus / AlertManager
# url="http://example.com/healthz"  # The URL that blackbox will scrape
# interval="60s"                    # Scraping interval. Overrides value set in `defaults`
# scrapeTimeout="60s"               # Scrape timeout. Overrides value set in `defaults`
# module = "http_2xx"               # module to be used. Can be "http_2xx" (default), "icmp" or "tcp_connect".

# If at least one common_label is defined, Prometheus will be created with selectors
# matching these labels and only ServiceMonitors that meet the criteria of the selector,
# i.e. are labeled accordingly, are included by Prometheus.
# The LCM takes care that all ServiceMonitor created by itself are labeled accordingly.
# The key can not be "release" as that one is already used by the Prometheus helm chart.
#
#[k8s-service-layer.prometheus.common_labels]
#managed-by = "yaook-k8s"

# ANCHOR_END: ksl_prometheus_configuration

# ANCHOR: ksl_cert_manager_configuration
# --- KUBERNETES SERVICE LAYER : CERT MANAGER ---
# ansible prefix: "k8s_cert_manager_"
[k8s-service-layer.cert-manager]
# Enable management of a cert-manager.io instance
enabled = false # •ᴗ•

# Configure in which namespace the cert-manager is run. The namespace is
# created automatically, but never deleted automatically.
#namespace = "k8s-svc-cert-manager"

# Install or uninstall cert manager. If set to false, the cert-manager will be
# uninstalled WITHOUT CHECK FOR DISRUPTION!
#install = true

# Scheduling key for the cert manager instance and its resources. Has no
# default.
#scheduling_key =

# If given, a *cluster wide* Let's Encrypt issuer with that email address will
# be generated. Requires an ingress to work correctly.
# DO NOT ENABLE THIS IN CUSTOMER CLUSTERS, BECAUSE THEY SHOULD NOT CREATE
# CERTIFICATES UNDER OUR NAME. Customers are supposed to deploy their own
# ACME/Let's Encrypt issuer.
#letsencrypt_email = "..."

# By default, the ACME issuer will let the server choose the certificate chain
# to use for the certificate. This can be used to override it.
#letsencrypt_preferred_chain = "..."

# The ingress class to use for responding to the ACME challenge.
# The default value works for the default k8s-service-layer.ingress
# configuration and may need to be adapted in case a different ingress is to be
# used.
#letsencrypt_ingress = "nginx"

# This variable let's you specify the endpoint of the ACME issuer. A common usecase
# is to switch between staging and production. [0]
# [0]: https://letsencrypt.org/docs/staging-environment/
# letsencrypt_server: https://acme-staging-v02.api.letsencrypt.org/directory

# ANCHOR_END: ksl_cert_manager_configuration

# ANCHOR: ksl_ingress_configuration
# --- KUBERNETES SERVICE LAYER : INGRESS ---
# ansible prefix: "k8s_ingress_"
[k8s-service-layer.ingress]
# Enable nginx-ingress management.
enabled = false # •ᴗ•

# Namespace to deploy the ingress in (will be created if it does not exist, but
# never deleted).
#namespace = "k8s-svc-ingress"

# If enabled, choose whether to install or uninstall the ingress. IF SET TO
# FALSE, THE INGRESS CONTROLLER WILL BE DELETED WITHOUT CHECKING FOR
# DISRUPTION.
#install = true

# Scheduling key for the cert manager instance and its resources. Has no
# default.
#scheduling_key =

# Service type for the frontend Kubernetes service.
#service_type = "LoadBalancer"

# Node port for the HTTP endpoint
#nodeport_http = 32080

# Node port for the HTTPS endpoint
#nodeport_https = 32443

# Enable SSL passthrough in the controller
#enable_ssl_passthrough = true

# Replica Count
#replica_count = 1

# Allow snippet annotations
#ingress_allow_snippet_annotations = false

# ANCHOR_END: ksl_ingress_configuration

# ANCHOR: etcd_backup_configuration
# --- KUBERNETES SERVICE LAYER : ETCD-BACKUP ---
# ansible prefix: "etcd_backup_"
[k8s-service-layer.etcd-backup]
#enabled = false

# Configure value for the cron job schedule for etcd backups. If not set it will be
# set to default value of  21 */12 * * *
#schedule = "21 * * * *"

# Name of the s3 bucket to store the backups. It defaults to `etcd-backup`
#bucket_name = "etcd-backup"

# Name of the folder to keep the backup files. It defaults to `etcd-backup`
#file_prefix = "backup"

# Configure the location of the Vault kv2 storage where the credentials can
# be found. This location is the default location used by import.sh and is
# recommended.
#vault_mount_point = "yaook/{{ vault_cluster_name }}/kv"

# Configure the kv2 key under which the credentials are found. This location is
# the default location used by import.sh and is recommended.
#
# The role expects a JSON object with `access_key` and `secret_key` keys,
# containing the corresponding S3 credentials.
#vault_path = "etcdbackup"

# Number of days after which individual items in the bucket are dropped. Enforced by S3 lifecyle rules which
# are also implemented by Ceph's RGW.
#days_of_retention = 30

# etcdbackup chart version to install.
# If this is not specified, the latest version is installed.
#chart_version=""

# Metrics port on which the backup-shifter Pod will provide metrics.
# Please note that the etcd-backup deployment runs in host network mode
# for easier access to the etcd cluster.
#metrics_port: 19100

# ANCHOR_END: etcd_backup_configuration

# ANCHOR: ksl_vault_configuration
# --- KUBERNETES SERVICE LAYER : HASHICORP VAULT ---
# ansible prefix: "yaook_vault_"
[k8s-service-layer.vault]
# Enable HashiCorp Vault management.
# WARNING: This is *NOT* production ready at the time of writing. The following
# shortcomings exist:
# - There are no backups.
# - There are no replicas.
# NOTE: On the first run, the unseal keys and the root token will be printed IN
# PLAINTEXT on the ansible output. The unseal keys MUST BE SAVED IN A SECURE
# LOCATION to use the Vault instance in the future!
#enabled = false # •ᴗ•

# Create a publically reachable ingress resource for the API endpoint of vault.
#ingress = false

# Version of the Helm Chart to use
#chart_version = "0.20.1"

# Namespace to deploy the vault in (will be created if it does not exist, but
# never deleted).
#namespace = "k8s-svc-vault"

# Extra DNS names for which certificates should be prepared.
# NOTE: to work correctly, there must exist an ingress of class `nginx` and it
# must allow ssl passthrough.
#dnsnames = [..]

# If set to true, the Vault is configured to be exposed via yaook/operator
# infra-ironic, that is, via the integrated DNSmasq to all nodes associated.
# The default is false. This can be enabled in non-infra-ironic clusters,
# without significant damage.
# NOTE: To work in infra-ironic clusters, this requires the vault to be in the
# same namespace as the infra-ironic instance.
# NOTE: if you enable this, you MUST NOT set the service_type to ClusterIP; it
# will default to NodePort and it must be at least NodePort or LoadBalancer for
# the integration to work correctly.
#management_cluster_integration = false

# Number of unseal key shares to generate upon vault initialization.
# NOTE: On the first run, the unseal keys and the root token will be printed IN
# PLAINTEXT on the ansible output. The unseal keys MUST BE SAVED IN A SECURE
# LOCATION to use the Vault instance in the future!
#init_key_shares = 5

# Threshold for the Shamir's Secret Sharing Scheme used for unsealing, i.e. the
# number of shares required to unseal the vault after a restart
# NOTE: On the first run, the unseal keys and the root token will be printed IN
# PLAINTEXT on the ansible output. The unseal keys MUST BE SAVED IN A SECURE
# LOCATION to use the Vault instance in the future!
#init_key_threshold = 2

# Scheduling key for the vault instance and its resources. Has no default.
#scheduling_key =

# Storage class for the vault file storage backend.
#storage_class = "csi-sc-cinderplugin"

# Storage size for the vault file storage backend.
#storage_size = "8Gi"

# If `ingress=True` and `dnsnames` is not empty, you have to tell the LCM which (Cluster)Issuer to use
# for your ACME service.
#external_ingress_issuer_name = ""

# Can be `Issuer` or `ClusterIssuer`, depending on the kind of issuer you would like
# to use for externally facing certificates.
#external_ingress_issuer_kind = "ClusterIssuer"

# If `true`, then an additional backup service will be deployed which creates snapshots and stores
# them in an S3 bucket.
#enable_backups = true

# Credentials to access an S3 bucket to which the backups will be written. Required if `enable_backups = true`.
# You can find a template in `managed-k8s/templates/vault_backup_s3_config.template.yaml`.
#s3_config_file = "vault_backup_s3_config.yaml"

# Type of the Kubernetes Service of the Vault
# NOTE: You may set this to LoadBalancer, but note that this will still use the internal certificate.
# If you want to expose the Vault to the outside world, use the ingress config above.
#service_type = "ClusterIP"

# Node port to use for the Service which exposes the active Vault instance
# See NOTE above regarding exposure of the Vault.
#active_node_port = 32048

# ANCHOR_END: ksl_vault_configuration

# ANCHOR: ksl_fluxcd_configuration
# --- KUBERNETES SERVICE LAYER : FLUXCD ---
# ansible prefix: "fluxcd_"
[k8s-service-layer.fluxcd]
# Enable Flux management.
#enabled = false

# Version of fluxcd to be deployed. Currently supported is `v0.36.0`.
#version = "v0.36.0"

# Namespace to deploy the flux-system in (will be created if it does not exist, but
# never deleted).
#namespace = "k8s-svc-flux-system"

# ANCHOR: node_scheduling_configuration
# --- NODE SCHEDULING ---
# ansible prefix: /
[node-scheduling]
# Scheduling keys control where services may run. A scheduling key corresponds
# to both a node label and to a taint. In order for a service to run on a node,
# it needs to have that label key. The following defines a prefix for these keys
scheduling_key_prefix = "scheduling.mk8s.cloudandheat.com"

# --- NODE SCHEDULING: LABELS (sent to ansible as k8s_node_labels!) ---
[node-scheduling.labels]
# Labels are assigned to a node during its initialization/join process only!
#
# The following fields are commented out because they make assumptions on the existence
# and naming scheme of nodes. Use them for inspiration :)
#managed-k8s-worker-0 = ["{{ scheduling_key_prefix }}/storage=true"]
#managed-k8s-worker-1 = ["{{ scheduling_key_prefix }}/monitoring=true"]
#managed-k8s-worker-2 = ["{{ scheduling_key_prefix }}/storage=true"]
#managed-k8s-worker-3 = ["{{ scheduling_key_prefix }}/monitoring=true"]
#managed-k8s-worker-4 = ["{{ scheduling_key_prefix }}/storage=true"]
#managed-k8s-worker-5 = ["{{ scheduling_key_prefix }}/monitoring=true"]
#
# --- NODE SCHEDULING: TAINTS (sent to ansible as k8s_node_taints!) ---
[node-scheduling.taints]
# Taints are assigned to a node during its initialization/join process only!
#
# The following fields are commented out because they make assumptions on the existence
# and naming scheme of nodes. Use them for inspiration :)
#managed-k8s-worker-0 = ["{{ scheduling_key_prefix }}/storage=true:NoSchedule"]
#managed-k8s-worker-2 = ["{{ scheduling_key_prefix }}/storage=true:NoSchedule"]
#managed-k8s-worker-4 = ["{{ scheduling_key_prefix }}/storage=true:NoSchedule"]

# ANCHOR_END: node_scheduling_configuration

# ANCHOR: wireguard_config
# --- WIREGUARD ---
# ansible prefix: "wg_"
[wireguard]
# Set the environment variable "WG_COMPANY_USERS" or this field to 'false' if C&H company members
# should not be rolled out as wireguard peers.
#rollout_company_users = false

# IP address range to use for WireGuard clients. Must be set to a CIDR and must
# not conflict with the terraform.subnet_cidr.
# Should be chosen uniquely for all clusters of a customer at the very least
# so that they can use all of their clusters at the same time without having
# to tear down tunnels.
ip_cidr = "172.30.153.64/26"
ip_gw   = "172.30.153.65/26"

# Same for IPv6
#ipv6_cidr = "fd01::/120"
#ipv6_gw = "fd01::1/120"

port = 7777 # •ᴗ•

# To add WireGuard keys, create blocks like the following
# You can add as many of them as you want. Inventory updater will auto-allocate IP
# addresses from the configured ip_cidr.
#[[wireguard.peers]]
#pub_key = "test1"
#ident = "testkunde1"

## Wireguard-based site-to-site tunnel
# If enabled, configure site to site tunnel
#s2s_enabled = false

# Subnet of the wireguard "transfer net" between the two endpoints
#s2s_transfer_subnet = "172.30.18.2/31"

# IP which is assigned to our VRRP master in transfer network
#s2s_ip = "172.30.18.2"

# IP which is assigned to our VRRP master in transfer network
#s2s_peer_ip = "172.30.18.3"

# Port on which wireguard listens
#s2s_port = "16000"

# Public wireguard key of the peer
#s2s_peer_pub_key = "7CuC/cSw1US+nilx0ihoA1qb2DsQI0QV2RBuLE8cnhk="

# Endpoint under which the peer can be reached
#s2s_peer_public_endpoint = "<public-IP-of-your-peer>:16000"

# BGP AS IDs for both parties (should differ, unless iBGP is wanted)
#s2s_bgp_as = "65010"
#s2s_peer_bgp_as = "65009"

# ANCHOR_END: wireguard_config

# ANCHOR: ipsec_configuration
# --- IPSEC ---
# ansible prefix: "ipsec_"
[ipsec]
# enabled = false

# Flag to enable the test suite.
# Must make sure a remote endpoint, with ipsec enabled, is running and open for connections.
# test_enabled = false

# Must be a list of parent SA proposals to offer to the client.
# Must be explicitly set if ipsec_enabled is set to true.
#proposals =

# Must be a list of ESP proposals to offer to the client.
#esp_proposals = "{{ ipsec_proposals }}"

# List of CIDRs to route to the peer. If not set, only dynamic IP
# assignments will be routed.
#peer_networks = []

# List of CIDRs to offer to the peer.
#local_networks = ["{{ subnet_cidr }}"]

# Pool to source virtual IP addresses from. Those are the IP addresses assigned
# to clients which do not have remote networks. (e.g.: "10.3.0.0/24")
#virtual_subnet_pool = null

# List of addresses to accept as remote. When initiating, the first single IP
# address is used.
#remote_addrs = false

# Private address of remote endpoint.
# only used when test_enabled is True
#remote_private_addrs = ""

# The PSK for EAP. Must be set.
#eap_psk =

# ANCHOR_END: ipsec_configuration

# ANCHOR: passwordstore_configuration
# --- PASSWORDSTORE ---
# ansible prefix: "passwordstore_"
[passwordstore]
# Set this field to `true` if the "company" users should be rolled out.
#rollout_company_users = false

# Configure Additional GPG-IDs that should have access to the cluster-repo specific passwordstore.
# If you're not member of the "company-wide" list, e.g., because you're a student you must add yourself here.
# yannic.ahrens@cloudandheat.com serves as an example here because the author is incredibly vain and likes
# to see his name written down everywhere.
# Parameters:
#   - 'ident': easy to read identification string, not used anywhere yet
#   - 'gpg_id': ID of your public GPG key, ideally in long-form

# --- PASSWORDSTORE: ADDITIONAL USERS ---
#[[passwordstore.additional_users]]
#ident = "yannic.ahrens@cloudandheat.com"
#gpg_id = "68AA582E81AD111C127F01273370EBE296354805"

# ANCHOR_END: passwordstore_configuration

# ANCHOR: ch-role-users_configuration
# --- C&H USERS ---
# ansible prefix: "cah_users_"
[cah-users]
rollout = false # •ᴗ•

# Include and exclude C&H users from rollout
# C&H users refers to items in the ch-users-databag repository
# For additional information refer the ch-role-users repository
# The users have to be provided as List<String>
# Possible Configurations:
# - roll_out_users_from
#   - include specific user group
#   - Default: ["opsi", "it-operations", "head"]
# - exclude_users_from
#   - exclude specific user group
#   - Default: ["students", "service"]
# - include_users
#   - include specific user(s)
#   - Default: []
# - exclude_users
#   - exclude specific user(s)
#   - Default: ["deployer"]
# Example Usage:
# include_users = ["<user>", "<user>"]

# ANCHOR_END: ch-role-users_configuration

# ANCHOR: testing_test_nodes_configuration
# --- TESTING: TEST NODES ---
[testing.test-nodes]
# The following fields are commented out because they make assumptions on the existence
# and naming scheme of nodes. Use them for inspiration :)
#"managed-k8s-worker-1" = "worker0"
#"managed-k8s-worker-3" = "worker1"
#"managed-k8s-worker-5" = "worker2"

# ANCHOR_END: testing_test_nodes_configuration

# ANCHOR: custom_configuration
# --- CUSTOM ---
# ansible prefix: /
# Specify variables to be used in the custom stage here. See below for examples.

##[custom]
#my_var_foo = "" # makes the variable `my_custom_section_prefix_my_var = ""`

#[custom.my_custom_section_prefix]
#my_var = "" # produces the var `my_custom_section_prefix_my_var = ""`

# ANCHOR_END: custom_configuration

# ANCHOR: miscellaneous_configuration
# --- MISCELLANEOUS ---
# ansible prefix: /
[miscellaneous]
# Install wireguard on all workers (without setting up any server-side stuff)
# so that it can be used from within Pods.
wireguard_on_workers = false

# Configuration details if the cluster will be placed behind a HTTP proxy.
# If unconfigured images will be used to setup the cluster, the updates of
# package sources, the download of docker images and the initial cluster setup will fail.
# NOTE: These chances are currently only tested for Debian-based operating systems and not for RHEL-based!
#cluster_behind_proxy = false
# Set the approriate HTTP proxy settings for your cluster here. E.g. the address of the proxy or
# internal docker repositories can be added to the no_proxy config entry
# Important note: Settings for the yaook-k8s cluster itself (like the service subnet or the pod subnet)
# will be set automagically and do not have to set manually here.
#http_proxy = "http://proxy.example.com:8889"
#https_proxy = "http://proxy.example.com:8889"
#no_proxy = "localhost,127.0.0.0/8"

# Name of the internal OpenStack network. This field becomes important if a VM is
# attached to two networks but the controller-manager should only pick up one. If
# you don't understand the purpose of this field, there's a very high chance you
# won't need to touch it/uncomment it.
# Note: This network name isn't fetched automagically (by terraform) on purpose
# because there might be situations where the CCM should not pick the managed network.
#openstack_network_name = "managed-k8s-network"

# Value for the kernel parameter `vm.max_map_count` on k8s worker nodes. Modifications
# might be required depending on the software running on the nodes (e.g., ElasticSearch).
# If you leave the value commented out you're fine and the system's default will be kept.
#vm_max_map_count = 262144

# Custom Docker Configuration
# A list of registry mirrors can be configured as a pull through cache to reduce
# external network traffic and the amount of docker pulls from dockerhub.
#docker_registry_mirrors: [ "https://0.docker-mirror.example.org", "https://1.docker-mirror.example.org" ]

# A list of insecure registries that can be accessed without TLS verification.
#docker_insecure_registries: [ "0.docker-registry.example.org", "1.docker-registry.example.org" ]

# Mirror Configuration for Containerd
# container_mirror_default_host = "install-node"
# container_mirrors = [
#   { name = "docker.io",
#     upstream = "https://registry-1.docker.io/",
#     port = 5000 },
#   { name = "gitlab.cloudandheat.com",
#     upstream = "https://registry.gitlab.cloudandheat.com/",
#     mirrors = ["https://install-node:8000"] },
# ]


# Custom Chrony Configration
# The ntp servers used by chrony can be customized if it should be necessary or wanted.
# A list of pools and/or servers can be specified.
# Chrony treats both similarily but it expects that a pool will resolve to several ntp servers.
#custom_chrony_configuration = false
#custom_ntp_pools = [ "0.pool.ntp.example.org", "1.pool.ntp.example.org"]
#custom_ntp_servers = [ "0.server.ntp.example.org", "1.server.ntp.example.org"]

# OpenStack credential checks
# Terrible things will happen when certain tasks are run and OpenStack credentials are not sourced.
# Okay, maybe not so terrible after all, but the templates do not check if certain values exist.
# Hence config files with empty credentials are written. The LCM will execute a simple check to see
# if you provided valid credentials as a sanity check iff you're on openstack and the flag below is set
# to True.
#check_openstack_credentials = True

# APT Proxy Configuration
# As a secondary effect, https repositories are not used, since
# those don't work with caching proxies like apt-cacher-ng.
# apt_proxy_url = "..."

# Custom PyPI mirror
# Use this in offline setups or to use a pull-through cache for
# accessing the PyPI.
# If the TLS certificate used by the mirror is not signed by a CA in
# certifi, you can put its cert in `config/pip_mirror_ca.pem` to set
# it explicitly.
# pip_mirror_url = "..."

[nvidia.vgpu]
# vGPU Support
# If virtualize_gpu in the [kubernetes] section is set to true, please also set these variables:
# driver_blob_url should point to a object store or otherwise web server, where the vGPU Manager installation file is available.
# driver_blob_url= "..."
# manager_filename should hold the name of the vGPU Manager installation file.
# manager_filename = "..."

# ANCHOR_END: miscellaneous_configuration
