---
- name: Check prometheus endpoint status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='{{ monitoring_prometheus_stack_release_name }}-kube-prom-prometheus'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check alertmanager endpoint status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{service='{{ monitoring_prometheus_stack_release_name }}-kube-prom-alertmanager'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check grafana endpoint status
  when: monitoring_use_grafana
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{service='{{ monitoring_prometheus_stack_release_name }}-grafana'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check k8s API endpoints status
  when: monitoring_use_grafana
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='apiserver',service='kubernetes',instance='kubernetes.default.svc:443'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check the kubelet endpoints status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kubelet',node='{{ item }}'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop: "{{ groups['k8s_nodes'] }}"

- name: Check the kube controller manager endpoints status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kube-controller-manager',nodename='{{ item }}'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop: "{{ groups['masters'] }}"

- name: Check the coreDNS endpoint status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='coredns',service='{{ monitoring_prometheus_stack_release_name }}-kube-prom-coredns'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check the kube scheduler endpoints status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kube-scheduler',nodename='{{ item }}'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop: "{{ groups['masters'] }}"

- name: Check the kube proxy endpoints status
  when: k8s_network_plugin in ['calico']
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kube-proxy',nodename='{{ item }}'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop: "{{ groups['k8s_nodes'] }}"

- name: Check the kube-state-metrics endpoint status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kube-state-metrics'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check k8s node exporters endpoints status
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='node-exporter',nodename='{{ item }}'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop: "{{ groups['k8s_nodes'] }}"

- name: Check scraping of etcd (kube-etcd)
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g "http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job='kube-etcd'}"
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3

- name: Check thanos (v1) endpoints
  when: monitoring_use_thanos | bool and not monitoring_use_helm_thanos | bool
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g 'http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job={{ item.job | to_json }},container={{ item.container | to_json }}}'
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop:
    # Look for both job and container. The thanos-sidecar is to blame because I don't consider 'prometheus-stack-kube-prom-thanos-discovery'
    # to be a descriptive name for a ServiceMonitor :)
    - job: thanos-compact
      container: thanos-compact
    - job: thanos-query
      container: thanos-query
    - job: thanos-store
      container: thanos-store
    - job: prometheus-stack-kube-prom-thanos-discovery
      container: thanos-sidecar

- name: Check thanos (v2) endpoints
  when: monitoring_use_thanos | bool and monitoring_use_helm_thanos | bool
  kubernetes.core.k8s_exec:
    namespace: "{{ check_prometheus_scraping_namespace }}"
    pod: prometheus-test-scraper
    command: curl -g 'http://{{ prometheus_cluster_ip }}:{{ prometheus_web_port }}/api/v1/query?query=up{job={{ item.job | to_json }},container={{ item.container | to_json }}}'
  register: metrics
  until: |
    metrics.failed == false and
    metrics.rc == 0 and
    metrics.stdout | from_json | json_query('status') == "success" and
    metrics.stdout | from_json | json_query('data.result[0].value | [1]') == '1'
  retries: 60
  delay: 3
  loop:
    # Look for both job and container. The thanos-sidecar is to blame because I don't consider 'prometheus-stack-kube-prom-thanos-discovery'
    # to be a descriptive name for a ServiceMonitor :)
    - job: thanos-compactor
      container: compactor
    - job: thanos-query
      container: query
    - job: thanos-storegateway
      container: storegateway
    - job: prometheus-stack-kube-prom-thanos-discovery
      container: thanos-sidecar
...
