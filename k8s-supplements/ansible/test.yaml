---
- name: Check monitoring
  hosts: orchestrator
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  tags:
  - monitoring-test
  roles:
  - role: monitoring_v2_test
    tags:
    - monitoring_v2_test
    when: k8s_monitoring_enabled | bool

- name: Service Tests
  hosts: orchestrator
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  tags:
  - service-test
  roles:
  - role: cert_manager_v1_test
    when: k8s_cert_manager_enabled | bool
    tags:
    - cert_manager_v1_test

  - role: fluxcd2_v1_test
    when: (fluxcd_enabled is defined) and (fluxcd_enabled | bool)
    tags:
    - fluxcd2_v1_test

- name: Connect to Kubernetes nodes
  import_playbook: "{{ ansible_k8s_core_dir }}/connect-to-nodes.yaml"
  vars:
    target_hosts: k8s_nodes

- name: Fail if node got not bootstrapped once, yet
  any_errors_fatal: true
  hosts: k8s_nodes
  tasks:
  - name: Fail if node got not bootstrapped once, yet
    when: not ansible_local['bootstrap']['bootstrapped'] | default(False) | bool
    fail:
      msg: |
        ERROR

        We're at an advanced stage of the rollout,
        but the node did not get bootstrapped yet!
        Please ensure the k8s-core/bootstrap playbook
        is executed at least once against every node
        before proceeding.
        This is automatically done if the install-all
        playbook of either k8s-core or k8s-supplements is executed.

- name: Check K8s setup
  hosts: k8s_nodes
  become: false
  gather_facts: false
  tasks:
  - name: Get node info
    delegate_to: "{{ groups['orchestrator'] | first }}"
    kubernetes.core.k8s_info:
      kind: node
      name: "{{ inventory_hostname }}"
    register: result
    tags:
    - test-ccm

  - name: Check that all hosts are initialized by the cloud provider
    vars:
      taints: "{{ result | json_query('resources[*].spec.taints[*]') }}"
    ansible.builtin.fail:
    when: "'node.cloudprovider.kubernetes.io/uninitialized' in taints"
    tags:
    - test-ccm

  - name: Check that the Internal IP address of each node is set
    vars:
      ip_address: "{{ result | json_query('resources[*].status.addresses[*].address') | count }}"
    ansible.builtin.fail:
    when: "ip_address == '0'"
    tags:
    - test-ccm

- name: Validate Kubernetes network configuration
  hosts: k8s_nodes
  roles:
  - role: check-network-configuration
    tags:
    - test-network-configuration
    - check-network-configuration

- name: Test Calico CNI Support
  hosts: k8s_nodes
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  roles:
  - role: check-calico
    tags:
    - check-calico
    - test-calico
    - check-dualstack
    - test-dualstack
    when: k8s_network_plugin == 'calico'

- name: Validate IPSec
  hosts: orchestrator
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  roles:
  - role: check-ipsec
    tags:
    - check-ipsec
    - test-ipsec
    when: ipsec_enabled and ipsec_test_enabled

- name: Test Nvidia GPU Support
  hosts: orchestrator
  vars_files:
  - vars/retries.yaml
  roles:
  - role: check-nvidia-device-plugin
    when: k8s_is_gpu_cluster and not k8s_virtualize_gpu
    tags:
    - test-nvidia-device-plugin
    - check-nvidia-device-plugin

- name: Annotate worker nodes
  hosts: orchestrator
  become: false
  gather_facts: false
  tags:
  - label
  pre_tasks:
  - name: Fail if insufficient testing nodes specified
    when:
    - testing_nodes is defined
    - testing_nodes | length < 2
    fail:
      msg: |
        Insufficient number of testing nodes specified.
        Please specify at least two testing nodes in
        your config under "testing.nodes".
    tags: always

  tasks:
  - name: Label testing nodes
    kubernetes.core.k8s:
      state: patched
      kind: Node
      name: "{{ item }}"
      definition:
        metadata:
          labels:
            k8s.yaook.cloud/test-node: "true"
    loop: "{{ testing_nodes }}"
    when: testing_nodes is defined
    tags:
    - test-services
    - check-services
    - test-block-storage-cinder
    - check-block-storage-cinder
    - test-block-storage-ceph
    - check-block-storage-ceph
    - test-ceph-shared-filesystem
    - check-ceph-shared-filesystem
    - test-shared-fs
    - check-shared-fs

- name: Check services
  hosts: orchestrator
  become: false
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  vars:
    gateway: "{{ groups['gateways'] | first or groups['masters'] | first }}"
    first_worker: "{{ groups['workers'] | first }}"
    deprecated_nodeport_lb_test_port: "{{ hostvars[groups['frontend'] | first ]['deprecated_nodeport_lb_test_port'] | default(0) }}"
  roles:
  - role: check-services
    when: "deprecated_nodeport_lb_test_port | int > 0"
    tags:
    - test-services
    - check-services
    - test-dualstack
    - check-dualstack
  - role: check-loadbalancer-service
    tags:
    - test-loadbalancer-service
    - check-loadbalancer-service
    - test-dualstack
    - check-dualstack
    when: openstack_lbaas or ch_k8s_lbaas_enabled

- name: Test enforcement of NetworkPolicies
  hosts: k8s_nodes[-1]
  become: false
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  roles:
  - role: check-networkpolicy-enforcement
    tags:
    - test-networkpolicy-enforcement
    - check-networkpolicy-enforcement
    when: "k8s_network_plugin in ['calico']"

# Same line for argumentation: especially rook is known to take a bit more time to boot up
- name: Check storage services
  hosts: orchestrator
  become: false
  gather_facts: false
  vars_files:
  - vars/retries.yaml
  vars:
    gateway: "{{ groups['gateways'] | first }}"
  tasks:
  - name: Check configured rook ceph pools having a storage class
    include_role:
      name: check-block-storage
    vars:
      # Derive the corresponding Kubernetes storage class name
      # as during their creation in
      # https://gitlab.com/yaook/k8s/-/blob/235abe162e9e47e22c2071889d4345d930893eb5/k8s-service-layer/roles/rook_v2/templates/cluster-values.yaml.j2#L260
      block_storage_class: "{{ '%s-%s' | format(rook_cluster_name, item.name) }}"
    loop: >
      {{ rook_pools
         | selectattr('create_storage_class', 'defined')
         | selectattr('create_storage_class', 'equalto', true)
      }}
    when: "k8s_storage_rook_enabled | bool"
    tags:
    - test-block-storage-ceph
    - check-block-storage-ceph
  roles:
  - role: check-block-storage
    vars:
      block_storage_class: csi-sc-cinderplugin
      volume_snapshot_class: csi-cinder-snapclass
    when: on_openstack
    tags:
    - test-block-storage-cinder
    - check-block-storage-cinder
  - role: check-shared-fs
    vars:
      fs_storage_class: rook-ceph-cephfs
    when: "(k8s_storage_rook_enabled | bool) and (rook_ceph_fs | bool)"
    tags:
    - test-ceph-shared-filesystem
    - check-ceph-shared-filesystem
    - test-shared-fs
    - check-shared-fs
  - role: check-local-storage
    vars:
      storageclass: "{{ k8s_local_storage_static_storageclass_name }}"
    when: k8s_local_storage_static_enabled
    tags:
    - test-local-storage-static
    - check-local-storage-static
  - role: check-local-storage
    vars:
      storageclass: "{{ k8s_local_storage_dynamic_storageclass_name }}"
    when: k8s_local_storage_dynamic_enabled
    tags:
    - test-local-storage-dynamic
    - check-local-storage-dynamic

- name: Remove worker node annotation
  hosts: orchestrator
  become: false
  gather_facts: false
  tags: label
  tasks:
  - name: Remove annotation of worker nodes  # noqa no-changed-when
    ansible.builtin.command: "kubectl label --overwrite nodes {{ item }} k8s.yaook.cloud/test-node-"
    loop: "{{ testing_nodes }}"
    when: testing_nodes is defined
    tags:
    - test-services
    - check-services
    - test-block-storage-cinder
    - check-block-storage-cinder
    - test-block-storage-ceph
    - check-block-storage-ceph
    - test-ceph-shared-filesystem
    - check-ceph-shared-filesystem
    - test-shared-fs
    - check-shared-fs
...
