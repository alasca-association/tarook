toolbox:
  enabled: true
  affinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json') | to_json }}
  tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json') | to_json }}

monitoring:
  enabled: {{ k8s_monitoring_enabled | bool }}
  createPrometheusRules: {{ k8s_monitoring_enabled | bool }}
  rulesNamespaceOverride: "{{ rook_namespace }}"

clusterName: "{{ rook_cluster_name }}"
cephClusterSpec:
{% if rook_custom_ceph_version is defined %}
  cephVersion:
    image: "quay.io/ceph/ceph:{{ rook_custom_ceph_version }}"
    allowUnsupported: true
{% endif %}
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: {{ rook_skip_upgrade_checks | to_json }}
  continueUpgradeAfterChecksEvenIfNotHealthy: {{ rook_skip_upgrade_checks | to_json }}
  waitTimeoutForHealthyOSDInMinutes: 10
  priorityClassNames:
    all: system-cluster-critical
  mon:
    count: {{ rook_nmons }}
    allowMultiplePerNode: {{ rook_mon_allow_multiple_per_node }}
{% if rook_mon_volume %}
    volumeClaimTemplate:
      spec:
        storageClassName: "{{ rook_mon_volume_storage_class }}"
        resources:
          requests:
            storage: "{{ rook_mon_volume_size }}"
{% endif %}
  mgr:
{% if rook_mgr_use_pg_autoscaler %}
    count: {{ rook_nmgrs }}
    modules:
      - name: pg_autoscaler
        enabled: true
{% endif %}
  dashboard:
    enabled: {{ rook_dashboard | bool }}
    ssl: true
{% if rook_version[1:] is version('1.9', '>=') and k8s_monitoring_enabled %}
    createPrometheusRules: true
{% endif %}

  network:
{% if rook_use_host_networking %}
    provider: host
{% endif %}
    # https://gitlab.com/yaook/k8s/-/merge_requests/1304#note_2041293843
    dualStack: false
{% if ipv4_enabled %}
    ipFamily: "IPv4"
{% elif ipv6_enabled %}
    ipFamily: "IPv6"
{% endif %}
  crashCollector:
    disable: false
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  placement:
    all:
      nodeAffinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json')['nodeAffinity'] | default(dict()) | to_json }}
      tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json') | to_json }}
{% if rook_mon_scheduling_key %}
    mon:
      nodeAffinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json')['nodeAffinity'] | default(dict()) | to_json }}
      tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json', template_vars=dict(scheduling_key="{{ rook_mon_scheduling_key }}")) | to_json }}
{% endif %}
{% if rook_mgr_scheduling_key %}
    mgr:
      nodeAffinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json')['nodeAffinity'] | default(dict()) | to_json }}
      tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json', template_vars=dict(scheduling_key="{{ rook_mgr_scheduling_key }}")) | to_json }}
{% endif %}
  annotations:
  labels:
# Workaround as we can't directly overwrite the created ServiceMonitors
# persistently currently: https://github.com/rook/rook/issues/9618
{% if k8s_monitoring_enabled | bool and monitoring_common_labels %}
    all:
{% for label_key, label_value in monitoring_common_labels.items() %}
      {{ label_key | to_json }}: {{ label_value | to_json }}
{% endfor %}
{% endif %}
  resources:
    mgr: {{ rook_mgr_resources | to_json }}
    mon: {{ rook_mon_resources | to_json }}
    osd: {{ rook_osd_resources | to_json }}
    prepareosd:
    mgr-sidecar:
    crashcollector:
    logcollector:
    cleanup:
  removeOSDsIfOutAndSafeToRemove: {{ rook_osd_autodestroy_safe }}
  storage: # cluster level storage configuration and selection
{% if on_openstack %}
    useAllNodes: false
    useAllDevices: false
{% else %}
    useAllNodes: {{ rook_use_all_available_nodes }}
    useAllDevices: {{ rook_use_all_available_devices }}
{% endif %}
    config:
      encryptedDevice: "{{ rook_encrypt_osds }}" # the default value for this option is "false"
    onlyApplyOSDPlacement: false
# Bare Metal custom configuration
{% if not on_openstack and not rook_use_all_available_nodes %}
    nodes:
{% for node in rook_nodes %}
    - name: {{ node.name | to_json }}
{% if not rook_use_all_available_devices %}
{% if node.devices %}
      devices:
{% for device_name, device_cfg in node.devices.items() %}
      - name: {{ device_name | to_json }}
        config:
          {{ device_cfg.config | to_nice_yaml | indent(10) }}
{% endfor %}
{% endif %}
{% endif %}
      config:
        {{ node.config | to_nice_yaml | indent(8) }}
{% endfor %}
{% endif %}
# OpenStack configuration
{% if on_openstack %}
    storageClassDeviceSets:
    - name: cinder
      count: {{ rook_nosds }}
      portable: true
      placement:
{% if rook_osd_anti_affinity %}
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - rook-ceph-osd
                - key: app
                  operator: In
                  values:
                  - rook-ceph-osd-prepare
              topologyKey: kubernetes.io/hostname
        topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - rook-ceph-osd
              - rook-ceph-osd-prepare
{% endif %}
        nodeAffinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json')['nodeAffinity'] | default(dict()) | to_json }}
        tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json') | to_json }}
      resources: {{ rook_osd_resources | to_json }}
      volumeClaimTemplates:
      - metadata:
          creationTimestamp: null
          name: data  # it is important that the template is called data for rook v1.3
        spec:
          resources:
            requests:
              storage: "{{ rook_osd_volume_size }}"
          storageClassName: "{{ rook_osd_storage_class }}"
          volumeMode: Block
          accessModes:
          - ReadWriteOnce
{% endif %}
  disruptionManagement:
    managePodBudgets: {{ rook_manage_pod_budgets | bool }}
    osdMaintenanceTimeout: 30
    machineDisruptionBudgetNamespace: openshift-machine-api

  # healthChecks
  # Valid values for daemons are 'mon', 'osd', 'status'
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    # Change pod liveness probe, it works for all mon,mgr,osd daemons
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false



cephBlockPools:
{% for item in rook_pools %}
  - name: "{{ item.name }}"
    spec:
      failureDomain: "{{ item.failure_domain }}"
{% if item.erasure_coded %}
      erasureCoded:
        dataChunks: {{ item.erasure_coded.data_chunks }}
        codingChunks: {{ item.erasure_coded.coding_chunks }}
{% else %}
      # For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.
      replicated:
        size: {{ item.replicated }}
        # Disallow setting pool with replica 1, this could lead to data loss without recovery.
        # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
        requireSafeReplicaSize: {{ ((item.replicated | int) != 1) | to_json }}
{% endif %}
      deviceClass: {{ item.device_class | to_json }}
      parameters:
        compression_mode: none
      mirroring:
        enabled: false
        mode: image
      statusCheck:
        mirror:
          disabled: false
          interval: 60s
      annotations:
    storageClass:
      enabled: {{ item.create_storage_class | bool }}
      name: {{ "%s-%s" | format(rook_cluster_name, item.name) | to_json }}
      isDefault: false
      reclaimPolicy: Delete

      parameters:
        # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
        imageFeatures: layering

        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

{% endfor %}

cephFileSystems:
{% if rook_ceph_fs %}
  - name: "{{ rook_ceph_fs_name }}"
    spec:
      metadataPool:
        replicated:
          size: {{ rook_ceph_fs_replicated }}
          requireSafeReplicaSize: {{ (rook_ceph_fs_replicated | int != 1) | to_json }}
        parameters:
          compression_mode: "none"
      dataPools:
        - failureDomain: host
          replicated:
            size: {{ rook_ceph_fs_replicated }}
            # Disallow setting pool with replica 1, this could lead to data loss without recovery.
            # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
            requireSafeReplicaSize: {{ (rook_ceph_fs_replicated | int != 1) | to_json }}
          parameters:
            compression_mode: "none"
      preserveFilesystemOnDelete: true
      metadataServer:
        activeCount: 1
        activeStandby: true
        placement:
          nodeAffinity: {{ lookup('template', 'roles/config/common_defaults_v1/templates/affinity.json')['nodeAffinity'] | default(dict()) | to_json }}
          tolerations: {{ lookup('template', 'roles/config/common_defaults_v1/templates/tolerations.json') | to_json }}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-mds
                # topologyKey: kubernetes.io/hostname will place MDS across different hosts
                topologyKey: kubernetes.io/hostname
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-mds
                  # topologyKey: */zone can be used to spread MDS across different AZ
                  topologyKey: topology.kubernetes.io/zone
        annotations:
        labels:
        resources: {{ rook_mds_resources }}
      mirroring:
        enabled: false
    storageClass:
      enabled: true
      isDefault: false
      name: {{ "%s-cephfs" | format(rook_cluster_name) | to_json }}
      pool: {{ "%s-data0" | format(rook_ceph_fs_name) | to_json }}

      reclaimPolicy: Delete
      allowVolumeExpansion: true

      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

        clusterID: {{ rook_cluster_name | to_json }}
        fsName: {{ rook_ceph_fs_name | to_json }}
        pool: {{ "%s-data0" | format(rook_ceph_fs_name) | to_json }}
        provisionVolume: "true"
{% endif %}

cephObjectStores: []
