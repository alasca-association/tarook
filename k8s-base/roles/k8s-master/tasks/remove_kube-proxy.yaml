---
- name: Check presence of kube-proxy
  become: yes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  kubernetes.core.k8s_info:
    api_version: apps/v1`
    namespace: kube-system
    name: kube-proxy
    kind: DaemonSet
  register: kube_proxy_ds

- name: Fail if disruptions are not allowed
  fail:
    msg: Would have to remove kube-proxy, but disruptions are not allowed.
  when: kube_proxy_ds.resources and not _allow_disruption

- name: Remove kube-proxy daemonset
  become: yes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  kubernetes.core.k8s:
    api_version: apps/v1
    kind: DaemonSet
    namespace: kube-system
    name: kube-proxy
    state: absent
    wait: yes
    validate:
      fail_on_error: yes
      strict: yes
  register: kube_proxy
  # Retry this task on failures
  until: kube_proxy is not failed
  retries: "{{ k8s_error_retries }}"
  delay: "{{ k8s_error_delay }}"

- name: Clean up iptables rules  # noqa no-handler
  delegate_to: "{{ item }}"
  loop: "{{ groups['k8s_nodes'] }}"
  become: yes
  run_once: yes
  when: kube_proxy is changed
  command:
  args:
    argv: ["docker", "run",
           "--rm",
           "--privileged",
           "-v", "/lib/modules:/lib/modules",
           "--net=host",
           "k8s.gcr.io/kube-proxy-amd64:v1.20.14", # [*]
           "kube-proxy", "--cleanup"]
  # [*] Huuh, why is the version pinned? Well, it is because the
  #     cleanup procedure does not work with each image.
  #     As we do include this task also during k8s upgrades,
  #     we don't want to risk catching one for which it does not
  #     and then having to manually fix the cluster as it will be
  #     in an intermediate (half upgraded) state.
  #     https://github.com/cloudnativelabs/kube-router/issues/760
