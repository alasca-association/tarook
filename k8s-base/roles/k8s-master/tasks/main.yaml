- name: Load admin credentials into interactive shells
  become: yes
  template:
    src: kubernetes-admin.sh.j2
    dest: /etc/profile.d/kubernetes-admin.sh
    owner: root
    group: root
    mode: u=rw,go=r

- name: Check reachability of local kube-apiserver
  wait_for:
    port: 6443
    timeout: 5
  ignore_errors: yes
  register: local_kube_apiserver_status

- name: Register the available frontend
  set_fact:
    available_frontend: "{{ groups['frontend'] | first }}"

- name: Check reachability of load-balanced kube-apiserver
  when: local_kube_apiserver_status is failed
  vars:
    host: "{{ networking_fixed_ip }}:{{ hostvars[available_frontend]['k8s_apiserver_frontend_port'] }}"
  uri:
    url: "https://{{ host }}"
    status_code: 403
    validate_certs: no # self-signed certificate
    timeout: 3
  register: lb_kube_apiserver_status
  until: lb_kube_apiserver_status.status == 403
  retries: 3
  ignore_errors: yes

- name: Make sure 'etc_dir' exists
  delegate_to: localhost
  become: no
  file:
    path: "{{ etc_dir }}"
    state: directory
    mode: 0755

- name: Create parent directories for PKI directories
  become: yes
  file:
    path: "{{ item }}"
    state: directory
    mode: u=rwx,go=rx
  loop:
  - /etc/kubernetes
  - /var/lib/kubelet

# The focus of the following two tasks is on fixing the permissions.
- name: Create PKI directories
  become: yes
  file:
    path: "{{ item }}"
    state: directory
    mode: u=rwx,go-rwx
  loop:
  - /etc/kubernetes/pki
  - /etc/kubernetes/pki/etcd
  - /var/lib/kubelet/pki

- name: Obtain credentials
  # We do this step on each node, so we have to use node credentials
  environment:
    ANSIBLE_HASHI_VAULT_URL: "{{ lookup('env', 'VAULT_ADDR') }}"
    ANSIBLE_HASHI_VAULT_CA_CERT: "{{ lookup('env', 'VAULT_CACERT') }}"
  vars:
    kubeconfig_api_server_url: "https://{{ networking_fixed_ip }}:{{ hostvars[available_frontend]['k8s_apiserver_frontend_port'] }}"
  block:
  - name: Read role-id
    become: yes
    command:
    args:
      argv:
      - cat
      - /etc/vault/role-id
    register: cat_role_id

  - name: Read secret-id
    become: yes
    command:
    args:
      argv:
      - cat
      - /etc/vault/secret-id
    register: cat_secret_id

  # This is based on the k8s best practices
  # https://kubernetes.io/docs/setup/best-practices/certificates/

  - name: Load service account key from Vault # noqa ignore-errors
    ignore_errors: yes
    set_fact:
      service_account_key: "{{ lookup('community.hashi_vault.vault_kv2_get', 'k8s/service-account-key', engine_mount_point=('%s/%s/kv' | format(vault_path_prefix, vault_cluster_name)), mount_point=vault_nodes_approle, auth_method='approle', role_id=cat_role_id.stdout, secret_id=cat_secret_id.stdout).data.data.private_key | b64decode }}"

  - name: Generate private key if necessary
    when: service_account_key is not defined
    delegate_to: localhost
    community.crypto.openssl_privatekey:
      return_content: true
      type: "RSA"
      path: "{{ etc_dir }}/sa.key"
      mode: u=rw,go-rwx
    register: generated_service_account_key

  - name: Delete generated private key from disk
    delegate_to: localhost
    file:
      state: absent
      path: "{{ etc_dir }}/sa.key"

  - name: Store generated key in vault
    when: service_account_key is not defined
    delegate_to: localhost
    community.hashi_vault.vault_write:
      auth_method: approle
      mount_point: "{{ vault_nodes_approle }}"
      role_id: "{{ cat_role_id.stdout }}"
      secret_id: "{{ cat_secret_id.stdout }}"
      path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/kv/data/k8s/service-account-key"
      data:
        data:
          private_key: "{{ generated_service_account_key.privatekey | b64encode }}"

  - name: Use generated service account key
    when: service_account_key is not defined
    set_fact:
      service_account_key: "{{ generated_service_account_key.privatekey }}"

  - name: Derive service account public key
    community.crypto.openssl_privatekey_info:
      content: "{{ service_account_key }}"
    register: service_account_key_info

  - name: Write service account private key
    become: yes
    copy:
      dest: /etc/kubernetes/pki/sa.key
      content: "{{ service_account_key }}"
      owner: root
      group: root
      mode: u=r

  - name: Write service account public key
    become: yes
    copy:
      dest: /etc/kubernetes/pki/sa.pub
      content: "{{ service_account_key_info.public_key }}"
      owner: root
      group: root
      mode: ugo=r

  - name: Fetch CA certificates
    set_fact:
      k8s_ca_cert: "{{ lookup('template', 'ca.crt.j2', template_vars={'ca_pki_name': 'k8s-pki'}) }}"
      k8s_front_proxy_ca_cert: "{{ lookup('template', 'ca.crt.j2', template_vars={'ca_pki_name': 'k8s-front-proxy-pki'}) }}"

  - name: Write k8s CA file
    become: yes
    copy:
      dest: /etc/kubernetes/pki/ca.crt
      owner: root
      group: root
      mode: ugo=r
      content: "{{ k8s_ca_cert }}"

  - name: Write k8s front proxy CA file
    become: yes
    copy:
      dest: /etc/kubernetes/pki/front-proxy-ca.crt
      owner: root
      group: root
      mode: ugo=r
      content: "{{ k8s_front_proxy_ca_cert }}"

  - name: Write etcd CA file
    become: yes
    vars:
      ca_pki_name: etcd-pki
    template:
      src: ca.crt.j2
      dest: /etc/kubernetes/pki/etcd/ca.crt
      owner: root
      group: root
      mode: ugo=r

  - name: Generate controller-manager kubeconfig
    include_tasks: mkkubeconfig.yaml
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      kubeconfig_title: controller-manager
      kubeconfig_destination: /etc/kubernetes/controller-manager.conf
      kubeconfig_issuer: system-masters_controllers
      kubeconfig_user: system:kube-controller-manager

  - name: Generate scheduler kubeconfig
    include_tasks: mkkubeconfig.yaml
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      kubeconfig_title: scheduler
      kubeconfig_destination: /etc/kubernetes/scheduler.conf
      kubeconfig_issuer: system-masters_controllers
      kubeconfig_user: system:kube-scheduler

  - name: Generate kubelet kubeconfig
    include_tasks: mkkubeconfig.yaml
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      kubeconfig_title: kubelet
      kubeconfig_destination: /etc/kubernetes/kubelet.conf
      kubeconfig_issuer: system-nodes_node
      kubeconfig_user: "system:node:{{ inventory_hostname }}"
      kubeconfig_external: true
      kubeconfig_keypair_path: /var/lib/kubelet/pki/kubelet-client-current.pem

  - name: Generate admin kubeconfig
    include_tasks: mkkubeconfig.yaml
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      kubeconfig_title: admin
      kubeconfig_destination: /etc/kubernetes/admin.conf
      kubeconfig_issuer: system-masters_admin
      kubeconfig_user: kubernetes-admin

  - name: Obtain etcd server certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: etcd server certificate
      get_cert_destination_crt: /etc/kubernetes/pki/etcd/server.crt
      get_cert_destination_key: /etc/kubernetes/pki/etcd/server.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/etcd-pki/issue/server"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "{{ inventory_hostname }}"
        alt_names: "{{ inventory_hostname }}"
        ip_sans: "{{ local_ipv4_address }},127.0.0.1,::1{% if local_ipv6_address | default(False) %},{{ local_ipv6_address }}{% endif %}"
        ttl: 8784h

  - name: Obtain etcd peer certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: etcd peer certificate
      get_cert_destination_crt: /etc/kubernetes/pki/etcd/peer.crt
      get_cert_destination_key: /etc/kubernetes/pki/etcd/peer.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/etcd-pki/issue/peer"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "{{ inventory_hostname }}"
        alt_names: "{{ inventory_hostname }}"
        ip_sans: "{{ local_ipv4_address }},127.0.0.1,::1{% if local_ipv6_address | default(False) %},{{ local_ipv6_address }}{% endif %}"
        ttl: 8784h

  - name: Obtain etcd healthcheck certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: etcd healthcheck certificate
      get_cert_destination_crt: /etc/kubernetes/pki/etcd/healthcheck-client.crt
      get_cert_destination_key: /etc/kubernetes/pki/etcd/healthcheck-client.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/etcd-pki/issue/healthcheck"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "{{ inventory_hostname }}"
        alt_names: "{{ inventory_hostname }}"
        ttl: 8784h

  - name: Obtain apiserver etcd client certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: apiserver etcd client certificate
      get_cert_destination_crt: /etc/kubernetes/pki/apiserver-etcd-client.crt
      get_cert_destination_key: /etc/kubernetes/pki/apiserver-etcd-client.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/etcd-pki/issue/kube-apiserver"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "{{ inventory_hostname }}"
        ttl: 8784h

  - name: Obtain apiserver frontend certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: apiserver frontend certificate
      get_cert_destination_crt: /etc/kubernetes/pki/apiserver.crt
      get_cert_destination_key: /etc/kubernetes/pki/apiserver.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/k8s-pki/issue/apiserver"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "kube-apiserver"
        alt_names: "{{ inventory_hostname }},{{ inventory_hostname }}.node.{{ vault_cluster_name }},kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.{{ vault_cluster_name }},kubernetes.default.svc.cluster.local"
        ip_sans: "{{ networking_fixed_ip }},{{ local_ipv4_address }},127.0.0.1,::1{% if networking_fixed_ip_v6 | default(False) %},{{ networking_fixed_ip_v6 }},{{ local_ipv6_address }}{% endif %},{{ k8s_network_service_subnet | ansible.utils.nthhost(1) }}"
        ttl: 8784h

  - name: Obtain apiserver kubelet client certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: apiserver kubelet client certificate
      get_cert_destination_crt: /etc/kubernetes/pki/apiserver-kubelet-client.crt
      get_cert_destination_key: /etc/kubernetes/pki/apiserver-kubelet-client.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/k8s-pki/issue/system-masters_apiserver"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "apiserver:{{ inventory_hostname }}.node.{{ vault_cluster_name }}"
        ttl: 8784h

  - name: Obtain apiserver front proxy client certificate
    include_role:
      name: vault-onboarded
      tasks_from: get_cert
    vars:
      vault_role_id: "{{ cat_role_id.stdout }}"
      vault_secret_id: "{{ cat_secret_id.stdout }}"
      get_cert_title: apiserver front proxy client certificate
      get_cert_destination_crt: /etc/kubernetes/pki/front-proxy-client.crt
      get_cert_destination_key: /etc/kubernetes/pki/front-proxy-client.key
      get_cert_vault_path: "{{ vault_path_prefix }}/{{ vault_cluster_name }}/k8s-front-proxy-pki/issue/apiserver"
      get_cert_owner: root
      get_cert_vault_data:
        common_name: "{{ inventory_hostname }}"
        ttl: 8784h

- name: Delete secret data which is now in Vault
  file:
    state: absent
    path: /etc/kubernetes/pki/{{ item }}
  loop:
  - ca.key
  - front-proxy-ca.key
  - etcd/ca.key

- name: Ensure that /var/lib/etcd exists
  become: yes
  file:
    path: /var/lib/etcd
    state: directory
    mode: u=rwx,go-rwx

- name: Spawn K8s cluster with 'kubeadm init'
  when: "local_kube_apiserver_status is failed and lb_kube_apiserver_status is failed"
  tags:
    - spawn
  become: yes
  block:
  - name: Check if gcr.io can be reached
    command: kubeadm config images pull

  - name: Create kubeadm-init-config.yaml
    vars:
      gateway: "{{ groups['gateways'] | first }}"
    template:
      src: kubeadm-init-config.yaml.j2
      dest: /tmp/kubeadm-init-config.yaml
      owner: root
      group: root
      mode: 0600

  - name: Run kubeadm init
    # XXX: ansible command module does not support setting the umask
    shell: umask 077 && kubeadm init --node-name={{ inventory_hostname | quote }} --config=/tmp/kubeadm-init-config.yaml

  - name: Remove kubeadm-init-config.yaml
    file:
      path: /tmp/kubeadm-init-config.yaml
      state: absent

  - name: Configure bridge-nf-call-iptables
    sysctl:
      name: net.bridge.bridge-nf-call-iptables
      value: 1
      state: present

  - name: Copy kubeconfig to localhost
    fetch:
      src: /etc/kubernetes/admin.conf
      dest: "{{ etc_dir }}/"
      flat: yes

- name: Join the K8s control plane
  when: "local_kube_apiserver_status is failed and not lb_kube_apiserver_status is failed"
  become: yes
  environment:
    ANSIBLE_HASHI_VAULT_URL: "{{ lookup('env', 'VAULT_ADDR') }}"
    ANSIBLE_HASHI_VAULT_CA_CERT: "{{ lookup('env', 'VAULT_CACERT') }}"
  block:
    - name: Get certificate information
      community.crypto.x509_certificate_info:
        path: "/etc/kubernetes/pki/ca.crt"
      register: ca_cert

    - name: Create kubeadm-join-config.yaml
      template:
        src: kubeadm-join-config.yaml.j2
        dest: /tmp/kubeadm-join-config.yaml
        owner: root
        group: root
        mode: 0600

    - name: Join the existing cluster as another control plane node
      # XXX: ansible command module does not support setting the umask
      shell: "umask 077 && kubeadm join --config=/tmp/kubeadm-join-config.yaml --ignore-preflight-errors=FileAvailable--etc-kubernetes-kubelet.conf"

    - name: Remove kubeadm-join-config.yaml
      file:
        path: /tmp/kubeadm-join-config.yaml
        state: absent

- name: Enable periodic publishing of the k8s join key
  include_tasks: continuous_join_key.yaml
  when: k8s_continuous_join_key_enabled

- name: Ensure that the 'gods' group exists
  become: yes
  group:
    name: "{{ k8s_admin_credentials_group }}"
    state: present

- name: Make the kube config readable by gods
  become: yes
  file:
    state: file
    path: /etc/kubernetes/admin.conf
    owner: root
    group: "{{ k8s_admin_credentials_group }}"
    mode: u=rw,g=r,o-rwx

- name: Ensure that the current user is allowed to read the kube config
  become: yes
  user:
    append: yes
    groups: "{{ k8s_admin_credentials_group }}"
    name: "{{ ansible_user }}"
  when: "ansible_user | default(False)"

- name: Setup kube-router networking
  include_tasks: setup_kube-router.yaml
  when: k8s_network_plugin == 'kube-router'

- name: Fail if no networking was configured
  fail:
    msg: |
      You chose "{{ k8s_network_plugin }}", but that did not work.
      Currently, we support 'kube-router', and 'calico'.
  # This variable is set by the setup_*.yaml files included above.
  # If you reach this point, none of them was included, probably because of
  # a typo in your k8s_network_plugin variable.
  # If calico is chosen, the network setup will be done after setting up the k8s nodes
  when: not (runtime_k8s_networking_configured | default(False)) and not k8s_network_plugin == 'calico'

- name: Provide CA certificates as ConfigMap
  become: yes
  run_once: yes
  environment:
    ANSIBLE_HASHI_VAULT_URL: "{{ lookup('env', 'VAULT_ADDR') }}"
    ANSIBLE_HASHI_VAULT_CA_CERT: "{{ lookup('env', 'VAULT_CACERT') }}"
    KUBECONFIG: /etc/kubernetes/admin.conf
  k8s:
    apply: true
    validate:
      fail_on_error: true
      strict: true
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: cluster-ca-certs
        namespace: kube-system
      data:
        front-proxy-ca.crt: "{{ k8s_front_proxy_ca_cert }}"
        kubernetes-ca.crt: "{{ k8s_ca_cert }}"
