# A single Typha can support hundreds of Felix instances. That means we can
# safely scale it by the number of k8s nodes divided by fifty and ensure that
# at least two exist, if we have enough nodes for that
{% set node_bunches = (groups['k8s_nodes'] | length) / 50 %}
{% set target_number = node_bunches %}
{% set minimum_typhas = [2, node_bunches] | max %}
# more typhas than we have k8s masters makes no sense and is also impossible
# to schedule (once we actually prevent typhas from running on random
# nodes...), but it could happen on small clusters using the logic above.
{% set maximum_typhas = groups['masters'] | length %}
# now we pick the smallest number, because the maximum is a hard maximum and the minimum is a soft minimum
{% set replicas = [minimum_typhas, maximum_typhas] | min %}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    app.kubernetes.io/name: calico
    app.kubernetes.io/component: calico-typha
    app.kubernetes.io/part-of: calico-cni-plugin
spec:
  replicas: {{ replicas }}
  # In order for rolling upgrades of the typha to work correctly, we have to
  # ensure that k8s is allowed to kill a typha and that it avoids trying to
  # create more typhas than there are nodes available
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # the default is 25%, which in a three-node-cluster evaluates to 0 -> we
      # need to set it to 1
      maxUnavailable: 1
      # the default is 25%, which in a three-node-cluster evaluates to 0, but
      # that is actually fine. we use the maximum number of typhas minus the
      # number of replicas here, because that's the truth.
      maxSurge: {{ maximum_typhas - replicas }}
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: calico
      app.kubernetes.io/component: calico-typha
      app.kubernetes.io/part-of: calico-cni-plugin
  template:
    metadata:
      labels:
        app.kubernetes.io/name: calico
        app.kubernetes.io/component: calico-typha
        app.kubernetes.io/version: {{ image_versions.calico_version | string | to_json }}
        app.kubernetes.io/part-of: calico-cni-plugin
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # This is needed cause on a fresh setup, there is no "ready" node to schedulde to schedulde the Typha pods on
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      # Since Calico can't network a pod until Typha is up, we need to run Typha itself
      # as a host-networked pod.
      serviceAccountName: calico-typha
      priorityClassName: system-cluster-critical
      # fsGroup allows using projected serviceaccount tokens as described here kubernetes/kubernetes#82573
      securityContext:
        fsGroup: 65534
      containers:
      - image: {{ "calico/typha:v%s" | format(image_versions.calico_version) | to_json }}
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        envFrom:
        - configMapRef:
            # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.
            name: kubernetes-services-endpoint
            optional: true
        env:
          # Enable "info" logging by default. Can be set to "debug" to increase verbosity.
          - name: TYPHA_LOGSEVERITYSCREEN
            value: "info"
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: "none"
          - name: TYPHA_LOGSEVERITYSYS
            value: "none"
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: "kubernetes"
          - name: TYPHA_DATASTORETYPE
            value: "kubernetes"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          # Uncomment these lines to enable prometheus metrics. Since Typha is host-networked,
          # this opens a port on the host, which may need to be secured.
          #- name: TYPHA_PROMETHEUSMETRICSENABLED
          #  value: "true"
          #- name: TYPHA_PROMETHEUSMETRICSPORT
          #  value: "9093"
          # Location of the CA bundle Typha uses to authenticate calico/node; volume mount
          - name: TYPHA_CAFILE
            value: /calico-typha-ca/typhaca.crt
          # Common name on the calico/node certificate
          - name: TYPHA_CLIENTCN
            value: calico-node
          # Location of the server certificate for Typha; volume mount
          - name: TYPHA_SERVERCERTFILE
            value: /calico-typha-certs/typha.crt
          # Location of the server certificate key for Typha; volume mount
          - name: TYPHA_SERVERKEYFILE
            value: /calico-typha-certs/typha.key
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9098
            host: localhost
          periodSeconds: 30
          initialDelaySeconds: 30
          timeoutSeconds: 10
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9098
            host: localhost
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: calico-typha-ca
          mountPath: "/calico-typha-ca"
          readOnly: true
        - name: calico-typha-certs
          mountPath: "/calico-typha-certs"
          readOnly: true
      volumes:
      - name: calico-typha-ca
        configMap:
          name: calico-typha-ca
      - name: calico-typha-certs
        secret:
          secretName: calico-typha-certs
