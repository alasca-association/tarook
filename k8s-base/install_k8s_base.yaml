---
- name: Initial sanity checks
  hosts: localhost
  gather_facts: false
  vars_files:
  - vars/etc.yaml
  roles:
  - validate_configuration

- name: Initial node bootstrap
  hosts: k8s_nodes
  gather_facts: false
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/etc.yaml
  - vars/retries.yaml
  roles:
  - role: detect_user
    tags: detect_user
    any_errors_fatal: true
  - role: prepare-node
    tags: prepare-node
  - role: journald
    tags: journald
  - role: is-k8s-installed
    tags: update_system
  - role: configure-automatic-system-updates
    tags: configure-automatic-system-updates
  - role: monitoring-system-update-status
    tags: monitoring-system-update-status
  - role: disable-swap
    tags: disable-swap

- name: Disable SELinux
  hosts: k8s_nodes
  roles:
  - role: disable_selinux
    tags: disable_selinux
    when: ansible_os_family == 'RedHat'

- name: Harden SSH
  hosts: k8s_nodes
  become: true
  vars_files:
  - vars/ssh-hardening.yaml
  - vars/etc.yaml
  roles:
  # https://github.com/dev-sec/ansible-collection-hardening/
  - role: devsec.hardening.ssh_hardening
    tags: harden-ssh

- name: Update SSH known hosts after SSH hardening
  hosts: k8s_nodes
  serial: 1
  become: true
  vars_files:
  - vars/etc.yaml
  roles:
  - role: ssh-known-hosts
    # the devsec hardening role tinkers with them
    allow_host_key_changes: true
    tags:
    - harden-ssh
    - ssh-known-hosts

- name: Upgrade the system
  hosts: k8s_nodes
  gather_facts: false
  vars_files:
  - vars/disruption.yaml
  - vars/retries.yaml
  tasks:
  - name: Upgrade the system
    when: install_status == 'k8s_not_installed'
    ansible.builtin.include_role:
      name: update_system
    tags:
    - update_system

- name: Detect login for gateways
  hosts: gateways
  gather_facts: false
  vars_files:
  - vars/etc.yaml
  roles:
  - detect_user

- name: Prepare Gateways
  hosts: frontend
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  roles:
  - role: k8s-api-frontend
    tags: k8s-api-frontend
  - role: global-monitoring-frontend
    tags: k8s-global-monitoring  # ToDo: find better tag
    when: k8s_global_monitoring_enabled

- name: Rollout company users
  hosts: k8s_nodes
  gather_facts: true
  tasks:
  - name: Rollout company users
    when: cah_users_rollout
    ansible.builtin.include_role:
      name: ch-role-users
    tags:
    - ch-role-users

- name: Prepare the k8s nodes
  hosts: k8s_nodes
  gather_facts: true
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/retries.yaml
  pre_tasks:
  - name: Obtain installed packages
    ansible.builtin.package_facts:
      manager: auto
  - name: Fail when docker is installed but containerd configured
    ansible.builtin.fail:
      msg: |
        Docker is installed on this node, but containerd is configured as CRI!
    when: "'docker.io' in ansible_facts.packages"
  - name: Prevent installing containerd for GPU cluster
    ansible.builtin.fail:
      msg: Trying to used containerd in a GPU cluster – this does not work.
    when: k8s_is_gpu_cluster | default(False)
  - name: Fail if outdated containerd package is installed
    become: true
    when:
    - "'containerd' in ansible_facts.packages"
    - install_status == 'k8s_installed'
    - not _allow_disruption
    ansible.builtin.fail:
      msg: |
        The 'containerd' package is installed,
        but the 'containerd.io' package from the official
        docker repositories is needed. Please allow disruption
        or manually uninstall the 'containerd' package on all
        Kubernetes nodes.
  roles:
  - role: networking
    tags: networking
  - role: ntp
    tags: ntp
  - role: gpu-support
    tags: gpu-support
  - role: vgpu-support
    tags: vgpu-support
  - role: docker
    tags: docker
    when: container_runtime == 'docker'
  - role: containerd
    tags: containerd
    when: container_runtime == 'containerd'
  - role: kubeadm
    tags: kubeadm
  - role: remove_snap
    tags: remove_snap

# If DualStack is enabled, the k8s cluster needs to use calico as CNI plugin
# In Ansible, it is not possible to overwrite inventory variables, therefore we need to fail here
# and ask the user to adjust its configuration.
- name: Validate DualStack configurations
  hosts: localhost
  tasks:
  - name: Check if DualStack support is enabled, but CNI plugin is not set to calico
    ansible.builtin.fail:
      msg: |
        ### ERROR ###
        You have enabled DualStack support, but have not chosen calico as k8s network plugin.
        Please set 'k8s_network_plugin' to 'calico' in your config and rerun.
    when: dualstack_support and k8s_network_plugin != "calico"

- name: Clean up control plane
  hosts: masters
  gather_facts: true
  serial: 1
  roles:
  - role: etcd-clean-stale-members
    tags: etcd-clean-stale-members

- name: Spawn the K8s control plane
  hosts: masters
  gather_facts: true
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/etc.yaml
  - vars/retries.yaml
  # With serial we can specify the number of hosts on which the Playbook is
  # executed in parallel.
  # https://docs.ansible.com/ansible/latest/user_guide/playbooks_delegation.html
  # FIXME: Do we really want to copy our OpenStack user credentials like this?
  serial:
  - 1
  - "100%"
  roles:
  - role: k8s-master
    tags: k8s-master

- name: Install calico supporting services
  hosts: masters
  gather_facts: false
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/etc.yaml
  - vars/retries.yaml
  roles:
  - role: calico-control-plane
    tags:
    - calico
    - calico-control-plane
    when: k8s_network_plugin == 'calico'
  - role: calico-worker
    tags:
    - calico
    - calico-control-plane
    - calico-worker
    when: k8s_network_plugin == 'calico'
  post_tasks:
  # This is required for the k8s node to come up after the installation
  # of calico – but only when using containerd, go figure!
  - name: Restart containerd
    when:
    - container_runtime == 'containerd'
    - install_status == 'k8s_not_installed'
    become: true
    ansible.builtin.systemd:
      name: containerd
      state: restarted
      enabled: true

- name: Spawn and configure the k8s worker nodes
  hosts: workers
  gather_facts: true
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/etc.yaml
  - vars/retries.yaml
  roles:
  - role: k8s-worker
    tags: k8s-worker
  - role: kubelet-configuration
    tags: kubelet-configuration
  - role: calico-worker
    tags:
    - calico
    - calico-worker
    when: k8s_network_plugin == 'calico'
  post_tasks:
  # This is required for the k8s node to come up after the installation
  # of calico – but only when using containerd, go figure!
  - name: Restart containerd
    when:
    - container_runtime == 'containerd'
    - install_status == 'k8s_not_installed'
    become: true
    ansible.builtin.systemd:
      name: containerd
      state: restarted
      enabled: true

- name: Cleanup Calico binaries
  hosts: localhost
  gather_facts: false
  roles:
  - role: calico-cleanup
    tags: calico-cleanup
    when: k8s_network_plugin == 'calico'

- name: Spawn the OpenStack k8s Control Plane
  hosts: masters
  become: true
  gather_facts: false
  vars_files:
  - vars/disruption.yaml
  - vars/auto_generated_preamble.yaml
  - vars/retries.yaml
  run_once: true
  roles:
  - role: connect-k8s-to-openstack
    tags: connect-k8s-to-openstack
    when: on_openstack | default(False) | bool
  - role: ch-k8s-lbaas-controller
    tags:
    - ch-k8s-lbaas
    - ch-k8s-lbaas-controller

- name: Configure BGP and LBaaS on the gateways
  hosts: gateways
  become: true
  gather_facts: true
  vars_files:
  - vars/auto_generated_preamble.yaml
  - vars/retries.yaml
  roles:
  - role: k8s-bgp
    tags:
    - k8s-bgp
  - role: ch-k8s-lbaas-agent
    tags:
    - ch-k8s-lbaas
    - ch-k8s-lbaas-agent

- name: Configure the local-storage controllers
  hosts: k8s_nodes
  vars_files:
  - vars/auto_generated_preamble.yaml
  - vars/retries.yaml
  roles:
  - role: k8s-local-storage-controller
    tags: lsc
    when: k8s_local_storage_static_enabled
  - role: k8s-local-path-provisioner
    tags: lpp
    when: k8s_local_storage_dynamic_enabled

# yahrens: This play resides only transitionally here.
# See https://gitlab.com/yaook/k8s/-/issues/349 for details
- name: Configure vm.max_map_count
  hosts: workers
  gather_facts: false
  tasks:
  - name: Configure vm.max_map_count
    ansible.builtin.import_role:
      name: vm-max-map-count
    when: "vm_max_map_count is defined"
    tags:
    - vm-max-map-count
...
